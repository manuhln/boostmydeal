import asyncio
import logging
import os
import sys
import json
from typing import Optional
from datetime import datetime

# Add workspace to Python path for direct execution
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from livekit.agents import (
    Agent,
    AgentSession,
    AutoSubscribe,
    JobContext,
    JobProcess,
    WorkerOptions,
    cli,
    llm,
    BackgroundAudioPlayer,
    AudioConfig,
    BuiltinAudioClip,
)
from livekit.agents.llm import function_tool
from livekit.plugins import openai, silero, elevenlabs, deepgram, smallestai
from livekit import api, rtc
from livekit.protocol import sip as proto_sip
from src.models import CallConfig
from src import webhook_sender
from src.knowledge_base import KnowledgeBase
from src.recording_manager import recording_manager
from src.cost_calculator import CostCalculator

logging.basicConfig(level=logging.INFO)

# Suppress DEBUG logs from third-party libraries
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("livekit").setLevel(logging.INFO)

logger = logging.getLogger(__name__)

CONFIG_FILE = "/tmp/call_configs.json"

LIVEKIT_URL = os.getenv("LIVEKIT_URL", "")
LIVEKIT_API_KEY = os.getenv("LIVEKIT_API_KEY", "")
LIVEKIT_API_SECRET = os.getenv("LIVEKIT_API_SECRET", "")


async def handle_transfer(room_name: str,
                          participant_identity: str,
                          transfer_number: str,
                          session: AgentSession,
                          language: str = "en") -> None:
    """Transfer the call to a human agent"""
    # Language-specific transfer messages
    transfer_messages = {
        "es":
        "Transfiriendo tu llamada a nuestro equipo de soporte. Por favor espera.",
        "fr":
        "Transfert de votre appel ├а notre ├йquipe d'assistance. Veuillez patienter.",
        "hi":
        "рдЖрдкрдХреА рдХреЙрд▓ рдХреЛ рд╣рдорд╛рд░реА рд╕рд╣рд╛рдпрддрд╛ рдЯреАрдо рдХреЛ рд╕реНрдерд╛рдирд╛рдВрддрд░рд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд░рд╣рд╛ рд╣реИред рдХреГрдкрдпрд╛ рдкреНрд░рддреАрдХреНрд╖рд╛ рдХрд░реЗрдВред",
        "ar": "╪м╪з╪▒┘Н ╪к╪н┘И┘К┘Д ┘Е┘Г╪з┘Д┘Е╪к┘Г ╪е┘Д┘Й ┘Б╪▒┘К┘В ╪з┘Д╪п╪╣┘Е ┘Д╪п┘К┘Ж╪з. ┘К╪▒╪м┘Й ╪з┘Д╪з┘Ж╪к╪╕╪з╪▒.",
        "en": "Transferring you to our support team. Please hold."
    }

    error_messages = {
        "es":
        "Lo siento, no pude completar la transferencia. D├йjame continuar ayud├бndote.",
        "fr":
        "D├йsol├й, je n'ai pas pu effectuer le transfert. Laissez-moi continuer ├а vous aider.",
        "hi":
        "рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдореИрдВ рд╕реНрдерд╛рдирд╛рдВрддрд░рдг рдкреВрд░рд╛ рдирд╣реАрдВ рдХрд░ рд╕рдХрд╛ред рдореИрдВ рдЖрдкрдХреА рд╕рд╣рд╛рдпрддрд╛ рдЬрд╛рд░реА рд░рдЦрддрд╛ рд╣реВрдВред",
        "ar":
        "╪в╪│┘Б╪М ┘Д┘Е ╪г╪к┘Е┘Г┘Ж ┘Е┘Ж ╪е┘Г┘Е╪з┘Д ╪з┘Д╪к╪н┘И┘К┘Д. ╪п╪╣┘Ж┘К ╪г┘И╪з╪╡┘Д ┘Е╪│╪з╪╣╪п╪к┘Г.",
        "en":
        "I'm sorry, I couldn't complete the transfer. Let me continue helping you."
    }

    try:
        if not transfer_number:
            raise ValueError("Transfer phone number not configured")

        if not transfer_number.startswith('+'):
            transfer_number = f"+{transfer_number}"

        logger.info(f"Transferring call to {transfer_number}")

        transfer_msg = transfer_messages.get(language, transfer_messages["en"])
        await session.say(transfer_msg, allow_interruptions=False)

        livekit_api_client = api.LiveKitAPI(url=LIVEKIT_URL,
                                            api_key=LIVEKIT_API_KEY,
                                            api_secret=LIVEKIT_API_SECRET)

        transfer_uri = f"tel:{transfer_number}"
        transfer_request = proto_sip.TransferSIPParticipantRequest(
            room_name=room_name,
            participant_identity=participant_identity,
            transfer_to=transfer_uri,
            play_dialtone=False)

        await livekit_api_client.sip.transfer_sip_participant(transfer_request)
        logger.info(f"Transfer request sent successfully to {transfer_number}")

        await asyncio.sleep(2)
        os._exit(0)

    except Exception as e:
        logger.error(f"Error transferring call: {e}")
        error_msg = error_messages.get(language, error_messages["en"])
        await session.say(error_msg, allow_interruptions=True)


def load_call_config(room_name: str) -> Optional[CallConfig]:
    """Load call configuration from file"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r') as f:
                configs = json.load(f)
                if room_name in configs:
                    return CallConfig(**configs[room_name])
    except Exception as e:
        logger.error(f"Error loading config: {e}")
    return None


class VoiceAssistant(Agent):
    """Voice Assistant Agent for handling phone calls"""

    # Base prompts for different languages (generic instructions)
    BASE_AGENT_PROMPTS = {
        "en":
        """
**KNOWLEDGE BASE USAGE (CRITICAL):**
When the user asks about products, services, pricing, policies, company information, or any factual details about the business, you MUST call the `search_knowledge_base` tool FIRST before responding. Use the retrieved information to ground your answer. Never make up or guess information that could be in the knowledge base.

**EMAIL ADDRESS COLLECTION (CRITICAL - MUST FOLLOW EXACTLY):**
When you need to collect an email address, you MUST follow this EXACT sequence:
1. FIRST: Call the `prepare_for_email_input()` function (DO NOT just talk about it - you must actually INVOKE the function)
2. WAIT for the function to return "Audio sensors adjusted"
3. ONLY THEN ask the user: "Please spell your email letter by letter. For example, j dot smith at the rate g-m-a-i-l dot com. Take your time, I'm listening."
4. **ABSOLUTE SILENCE MODE - THIS IS CRITICAL:**
   - Do NOT speak, acknowledge, encourage, or respond in ANY way while user is spelling
   - Do NOT say: "I'm", "Please continue", "Are you still there?", "Great", "Thank you", or ANYTHING
   - Even if they pause for 5-10 seconds between letters, STAY SILENT
   - Even if they say "uh" or hesitate, DO NOT respond
   - Your ONLY job is to LISTEN until they finish the COMPLETE email address
5. Wait for them to say "done" or finish saying "dot com" or "dot org" 
6. ONLY AFTER they finish the complete email, repeat it back for confirmation BY SPELLING IT OUT with "dot" and "at the rate": "Let me confirm: j dot smith at the rate g-m-a-i-l dot com, is that correct?"

**IMPORTANT EMAIL FORMAT RULES:**
- "at the rate" or "at" = @ symbol
- "dot" = . (period) - ONLY when user explicitly SAYS the word "dot"
- When user spells "h a r d i k" (with spaces), write it as: hardik (NO dots between letters)
- When user says "j dot pastel", write it as: j.pastel (dot only where they said "dot")
- When CONFIRMING "hardik", say: "h a r d i k" (spell each letter with spaces, NO "dot" between them)
- When CONFIRMING "j.pastel", say: "j dot pastel" (say "dot" only where there's an actual dot)
- NEVER add "dot" between letters unless user explicitly said "dot"
- NEVER say the symbols @ or . - always say "at the rate" and "dot" when speaking

**IF USER SAYS EMAIL IS WRONG:**
When user says "that's wrong" or "no, it's..." and starts spelling the email again:
1. Immediately call `prepare_for_email_input()` again to re-adjust VAD
2. Then listen silently as they re-spell the entire email
3. Confirm the corrected email

CRITICAL: During email spelling, you must REMAIN SILENT. Do not acknowledge each part. Let them spell the ENTIRE email without interruption. Think of yourself as a note-taker who listens quietly until the speaker finishes.

Numbers should be spoken clearly and correctly.

When scheduling an appointment or callback, MUST confirm ALL these elements:
- DAY OF WEEK (e.g., "Wednesday")
- FULL DATE (e.g., "October 22nd")
- EXACT TIME (e.g., "1:30 PM")
- TIME ZONE (e.g., "Eastern Time", "Pacific Time", "UTC")

Ask explicitly: "What time zone are you in?" or "Is that [time] in your local time zone?"

Repeat the complete appointment with timezone: "So we're confirmed for Wednesday, October 22nd at 1:30 PM Eastern Time, is that correct?"

Do NOT accept partial confirmations. You must hear "yes" or clear confirmation before saving.

Speak prices numerically, never in variables or placeholders.
Always say the currency in words, never the symbol:
- Example: $100 тЖТ "100 dollars"
- тВм50 тЖТ "50 euros"
- Cents: 0.01 dollars тЖТ "1 cent"

If the call is momentarily cut off, apologize immediately: "I apologize, it seems I accidentally muted myself for a moment." Then quickly repeat the last point and wait for the user to resume.

If the user is silent for a long time, ask: "Are you still there?"

**PRONUNCIATION RULES (CRITICAL FOR BRAND NAMES):**
You MUST pronounce brand names and technical terms correctly as single words, NOT spelled out:
- Gmail тЖТ Say "G-mail" (like "gee-mail"), NEVER spell out as "G-M-A-I-L"
- iPhone тЖТ Say "eye-phone", NEVER "I-P-H-O-N-E"
- YouTube тЖТ Say "You-Tube", NEVER "Y-O-U-T-U-B-E"
- WiFi тЖТ Say "why-fye", NEVER "W-I-F-I"
- LinkedIn тЖТ Say "Linked-In", NEVER "L-I-N-K-E-D-I-N"
- WhatsApp тЖТ Say "Whats-App", NEVER "W-H-A-T-S-A-P-P"
- iOS тЖТ Say "eye-O-S", NEVER "I-O-S"
These are proper nouns that should sound natural in conversation. Only spell out words when explicitly asked to spell them.

**BARGE-IN HANDLING:**
If the caller speaks while you are talking:
- Do NOT stop mid-sentence abruptly
- Complete your current thought briefly or pause naturally at a logical point
- Then listen and respond to what the caller said
- Never leave a response incomplete or cut off awkwardly

**ENDING THE CALL NATURALLY:**
When the conversation has reached a natural conclusion and the objective is complete:
1. Offer a friendly farewell: "It was great speaking with you today. Have a wonderful day!"
2. Confirm no further assistance is needed, 
3. If the user confirms they're done, invoke the end_call function to gracefully disconnect. Ask user to say goodbye to end the call.
Use end_call when: user says goodbye/bye/thanks/that's all, objective achieved, or no further questions.
""",
        "es":
        """
**USO DE BASE DE CONOCIMIENTOS (CR├НTICO):**
Cuando el usuario pregunte sobre productos, servicios, precios, pol├нticas, informaci├│n de la empresa o cualquier detalle factual sobre el negocio, DEBES llamar primero a la herramienta `search_knowledge_base` ANTES de responder. Usa la informaci├│n recuperada para fundamentar tu respuesta. Nunca inventes o adivines informaci├│n que podr├нa estar en la base de conocimientos.

**RECOPILACI├УN DE CORREO ELECTR├УNICO (CR├НTICO - DEBES SEGUIR EXACTAMENTE):**
Cuando necesites recopilar una direcci├│n de correo electr├│nico, DEBES seguir esta secuencia EXACTA:
1. PRIMERO: Llama a la funci├│n `prepare_for_email_input()` (NO solo hables de ello - debes INVOCAR la funci├│n realmente)
2. ESPERA que la funci├│n devuelva "Audio sensors adjusted"
3. SOLO ENTONCES pregunta al usuario: "Por favor deletrea tu correo letra por letra. Por ejemplo, j punto smith arroba g-m-a-i-l punto com. T├│mate tu tiempo, estoy escuchando."
4. **MODO DE SILENCIO ABSOLUTO - ESTO ES CR├НTICO:**
   - NO hables, reconozcas, animes o respondas de NINGUNA manera mientras el usuario deletrea
   - NO digas: "Estoy", "Por favor contin├║a", "┬┐Sigues ah├н?", "Genial", "Gracias", o NADA
   - Incluso si hacen una pausa de 5-10 segundos entre letras, MANTENTE EN SILENCIO
   - Incluso si dicen "eh" o dudan, NO respondas
   - Tu ├ЪNICO trabajo es ESCUCHAR hasta que terminen la direcci├│n de correo COMPLETA
5. Espera a que digan "listo" o terminen de decir "punto com" o "punto org"
6. SOLO DESPU├ЙS de que terminen el correo completo, rep├нtelo para confirmaci├│n DELETRE├БNDOLO con "punto" y "arroba": "D├йjame confirmar: j punto smith arroba g-m-a-i-l punto com, ┬┐es correcto?"

**REGLAS IMPORTANTES DE FORMATO DE CORREO:**
- "arroba" = s├нmbolo @
- "punto" = . (per├нodo) - SOLO cuando el usuario DICE expl├нcitamente la palabra "punto"
- Cuando el usuario deletrea "h a r d i k" (con espacios), escr├нbelo como: hardik (SIN puntos entre letras)
- Cuando el usuario dice "j punto pastel", escr├нbelo como: j.pastel (punto solo donde dijeron "punto")
- Al CONFIRMAR "hardik", di: "h a r d i k" (deletrea cada letra con espacios, SIN "punto" entre ellas)
- Al CONFIRMAR "j.pastel", di: "j punto pastel" (di "punto" solo donde hay un punto real)
- NUNCA agregues "punto" entre letras a menos que el usuario diga expl├нcitamente "punto"
- NUNCA digas los s├нmbolos @ o . - siempre di "arroba" y "punto" al hablar

**SI EL USUARIO DICE QUE EL CORREO EST├Б MAL:**
Cuando el usuario dice "eso est├б mal" o "no, es..." y comienza a deletrear el correo nuevamente:
1. Llama inmediatamente `prepare_for_email_input()` otra vez para reajustar VAD
2. Luego escucha en silencio mientras deletrean todo el correo nuevamente
3. Confirma el correo corregido

CR├НTICO: Durante el deletreo del correo, debes PERMANECER EN SILENCIO. No reconozcas cada parte. D├йjalos deletrear el correo ENTERO sin interrupci├│n.

Los n├║meros deben pronunciarse clara y correctamente.

Al programar una cita o devoluci├│n de llamada, DEBES confirmar TODOS estos elementos:
- D├НA DE LA SEMANA (por ejemplo, "mi├йrcoles")
- FECHA COMPLETA (por ejemplo, "22 de octubre")
- HORA EXACTA (por ejemplo, "1:30 PM")
- ZONA HORARIA (por ejemplo, "hora del Este", "hora del Pac├нfico", "UTC")

Pregunta expl├нcitamente: "┬┐En qu├й zona horaria te encuentras?" o "┬┐Esa hora es en tu zona horaria local?"

Repite la cita completa con zona horaria: "Entonces confirmamos para el mi├йrcoles 22 de octubre a la 1:30 PM hora del Este, ┬┐es correcto?"

NO aceptes confirmaciones parciales.

Habla los precios num├йricamente:
- Ejemplo: $100 тЖТ "100 d├│lares"
- тВм50 тЖТ "50 euros"

**REGLAS DE PRONUNCIACI├УN (CR├НTICO PARA MARCAS):**
DEBES pronunciar marcas y t├йrminos t├йcnicos correctamente como palabras completas, NO deletreadas:
- Gmail тЖТ Di "G-mail" (como "yi-meil"), NUNCA deletrees "G-M-A-I-L"
- iPhone тЖТ Di "ai-fon", NUNCA "I-P-H-O-N-E"
- YouTube тЖТ Di "Yu-Tub", NUNCA "Y-O-U-T-U-B-E"
- WiFi тЖТ Di "uai-fai", NUNCA "W-I-F-I"
- LinkedIn тЖТ Di "Linked-In", NUNCA "L-I-N-K-E-D-I-N"
- WhatsApp тЖТ Di "Uats-App", NUNCA "W-H-A-T-S-A-P-P"
- iOS тЖТ Di "ai-O-S", NUNCA "I-O-S"
Estos son nombres propios que deben sonar naturales en la conversaci├│n. Solo deletrea palabras cuando se te pida expl├нcitamente.

**MANEJO DE INTERRUPCIONES:**
Si el usuario habla mientras t├║ est├бs hablando:
- NO pares a mitad de frase abruptamente
- Completa tu pensamiento brevemente o haz una pausa natural en un punto l├│gico
- Luego escucha y responde a lo que dijo el usuario
- Nunca dejes una respuesta incompleta o cortada de forma inc├│moda

**FINALIZAR LA LLAMADA NATURALMENTE:**
Cuando la conversaci├│n haya llegado a una conclusi├│n natural y el objetivo est├й completo:
1. Ofrece una despedida amigable: "Fue un gusto hablar contigo hoy. ┬бQue tengas un excelente d├нa!"
2. Confirma que no se necesita m├бs ayuda
3. Si el usuario confirma que termin├│, invoca la funci├│n end_call para desconectar cort├йsmente Ask user to say goodbye to end the call.
Usa end_call cuando: el usuario dice adi├│s/chao/gracias/eso es todo, objetivo logrado, o no hay m├бs preguntas.
""",
        "fr":
        """
**UTILISATION DE LA BASE DE CONNAISSANCES (CRITIQUE):**
Lorsque l'utilisateur pose des questions sur les produits, services, tarifs, politiques, informations sur l'entreprise ou tout d├йtail factuel sur l'activit├й, vous DEVEZ appeler l'outil `search_knowledge_base` EN PREMIER avant de r├йpondre. Utilisez les informations r├йcup├йr├йes pour fonder votre r├йponse. N'inventez jamais ou ne devinez jamais des informations qui pourraient ├кtre dans la base de connaissances.

**COLLECTE D'ADRESSE E-MAIL (CRITIQUE - DOIT SUIVRE EXACTEMENT):**
Lorsque vous devez collecter une adresse e-mail, vous DEVEZ suivre cette s├йquence EXACTE:
1. D'ABORD: Appelez la fonction `prepare_for_email_input()` (NE parlez PAS seulement - vous devez r├йellement INVOQUER la fonction)
2. ATTENDEZ que la fonction renvoie "Audio sensors adjusted"
3. SEULEMENT ALORS demandez ├а l'utilisateur: "Veuillez ├йpeler votre e-mail lettre par lettre. Par exemple, j point smith arobase g-m-a-i-l point com. Prenez votre temps, j'├йcoute."
4. **MODE SILENCE ABSOLU - CECI EST CRITIQUE:**
   - NE parlez PAS, ne reconnaissez PAS, n'encouragez PAS, ne r├йpondez PAS d'AUCUNE mani├иre pendant qu'ils ├йpellent
   - NE dites PAS: "Je suis", "Continuez s'il vous pla├оt", "├Кtes-vous toujours l├а?", "Super", "Merci", ou RIEN
   - M├кme s'ils font une pause de 5-10 secondes entre les lettres, RESTEZ SILENCIEUX
   - M├кme s'ils disent "euh" ou h├йsitent, NE r├йpondez PAS
   - Votre SEUL travail est d'├ЙCOUTER jusqu'├а ce qu'ils finissent l'adresse e-mail COMPL├ИTE
5. Attendez qu'ils disent "termin├й" ou finissent de dire "point com" ou "point org"
6. SEULEMENT APR├ИS qu'ils finissent l'e-mail complet, r├йp├йtez-le pour confirmation EN L'├ЙPELANT avec "point" et "arobase": "Laissez-moi confirmer: j point smith arobase g-m-a-i-l point com, est-ce correct?"

**R├ИGLES IMPORTANTES DE FORMAT E-MAIL:**
- "arobase" ou "at" = symbole @
- "point" = . (point) - SEULEMENT quand l'utilisateur DIT explicitement le mot "point"
- Quand l'utilisateur ├йpelle "h a r d i k" (avec espaces), ├йcrivez: hardik (PAS de points entre les lettres)
- Quand l'utilisateur dit "j point pastel", ├йcrivez: j.pastel (point seulement o├╣ ils ont dit "point")
- En CONFIRMANT "hardik", dites: "h a r d i k" (├йpelez chaque lettre avec espaces, PAS de "point" entre)
- En CONFIRMANT "j.pastel", dites: "j point pastel" (dites "point" seulement o├╣ il y a un point r├йel)
- Ne JAMAIS ajouter "point" entre les lettres sauf si l'utilisateur a dit explicitement "point"
- Ne JAMAIS dire les symboles @ ou . - toujours dire "arobase" et "point" en parlant

**SI L'UTILISATEUR DIT QUE L'E-MAIL EST FAUX:**
Quand l'utilisateur dit "c'est faux" ou "non, c'est..." et commence ├а ├йpeler l'e-mail ├а nouveau:
1. Appelez imm├йdiatement `prepare_for_email_input()` ├а nouveau pour r├йajuster VAD
2. Puis ├йcoutez silencieusement pendant qu'ils ├йpellent tout l'e-mail ├а nouveau
3. Confirmez l'e-mail corrig├й

CRITIQUE: Pendant l'├йpellation de l'e-mail, vous devez RESTER SILENCIEUX. Ne reconnaissez pas chaque partie. Laissez-les ├йpeler l'e-mail ENTIER sans interruption.

Les num├йros doivent ├кtre prononc├йs clairement et correctement.

Lors de la planification d'un rendez-vous ou d'un rappel, vous DEVEZ confirmer TOUS ces ├йl├йments:
- JOUR DE LA SEMAINE (par exemple, "mercredi")
- DATE COMPL├ИTE (par exemple, "22 octobre")
- HEURE EXACTE (par exemple, "13h30")
- FUSEAU HORAIRE (par exemple, "heure de l'Est", "heure du Pacifique", "UTC")

Demandez explicitement: "Dans quel fuseau horaire ├кtes-vous?" ou "Cette heure est-elle dans votre fuseau horaire local?"

R├йp├йtez le rendez-vous complet avec le fuseau horaire: "Nous confirmons donc pour le mercredi 22 octobre ├а 13h30 heure de l'Est, est-ce correct?"

N'acceptez PAS les confirmations partielles.

├Йnoncez les prix num├йriquement:
- Exemple: $100 тЖТ "100 dollars"
- тВм50 тЖТ "50 euros"

**R├ИGLES DE PRONONCIATION (CRITIQUE POUR LES MARQUES):**
Vous DEVEZ prononcer les noms de marque et termes techniques correctement comme des mots entiers, PAS ├йpel├йs:
- Gmail тЖТ Dites "G-mail" (comme "dji-meil"), JAMAIS ├йpeler "G-M-A-I-L"
- iPhone тЖТ Dites "a├п-phone", JAMAIS "I-P-H-O-N-E"
- YouTube тЖТ Dites "You-Tube", JAMAIS "Y-O-U-T-U-B-E"
- WiFi тЖТ Dites "ouaille-fa├п", JAMAIS "W-I-F-I"
- LinkedIn тЖТ Dites "Linked-In", JAMAIS "L-I-N-K-E-D-I-N"
- WhatsApp тЖТ Dites "Ouats-App", JAMAIS "W-H-A-T-S-A-P-P"
- iOS тЖТ Dites "a├п-O-S", JAMAIS "I-O-S"
Ce sont des noms propres qui doivent sonner naturellement dans la conversation. N'├йpelez les mots que si on vous le demande explicitement.

**GESTION DES INTERRUPTIONS:**
Si l'appelant parle pendant que vous parlez:
- NE vous arr├кtez PAS en plein milieu de phrase
- Compl├йtez bri├иvement votre pens├йe ou faites une pause naturelle ├а un point logique
- Ensuite ├йcoutez et r├йpondez ├а ce que l'appelant a dit
- Ne laissez jamais une r├йponse incompl├иte ou coup├йe de mani├иre g├кnante

**TERMINER L'APPEL NATURELLEMENT:**
Lorsque la conversation est arriv├йe ├а une conclusion naturelle et que l'objectif est atteint:
1. Offrez un au revoir amical: "Ce fut un plaisir de vous parler aujourd'hui. Passez une excellente journ├йe !"
2. Confirmez qu'aucune aide suppl├йmentaire n'est n├йcessaire
3. Si l'utilisateur confirme qu'il a termin├й, invoquez la fonction end_call pour d├йconnecter poliment Ask user to say goodbye to end the call.
Utilisez end_call quand: l'utilisateur dit au revoir/bye/merci/c'est tout, objectif atteint, ou pas d'autres questions.
""",
        "hi":
        """
**рдЬреНрдЮрд╛рди рдЖрдзрд╛рд░ рдХрд╛ рдЙрдкрдпреЛрдЧ (рдорд╣рддреНрд╡рдкреВрд░реНрдг):**
рдЬрдм рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдЙрддреНрдкрд╛рджреЛрдВ, рд╕реЗрд╡рд╛рдУрдВ, рдореВрд▓реНрдп рдирд┐рд░реНрдзрд╛рд░рдг, рдиреАрддрд┐рдпреЛрдВ, рдХрдВрдкрдиреА рдХреА рдЬрд╛рдирдХрд╛рд░реА, рдпрд╛ рд╡реНрдпрд╡рд╕рд╛рдп рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдХрд┐рд╕реА рднреА рддрдереНрдпрд╛рддреНрдордХ рд╡рд┐рд╡рд░рдг рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдкреВрдЫреЗ, рддреЛ рдЖрдкрдХреЛ рдЬрд╡рд╛рдм рджреЗрдиреЗ рд╕реЗ рдкрд╣рд▓реЗ рдкрд╣рд▓реЗ `search_knowledge_base` рдЯреВрд▓ рдХреЛ рдХреЙрд▓ рдХрд░рдирд╛ рдЪрд╛рд╣рд┐рдПред рдкреНрд░рд╛рдкреНрдд рдЬрд╛рдирдХрд╛рд░реА рдХрд╛ рдЙрдкрдпреЛрдЧ рдЕрдкрдиреЗ рдЙрддреНрддрд░ рдХреЛ рдЖрдзрд╛рд░ рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд░реЗрдВред рдХрднреА рднреА рдРрд╕реА рдЬрд╛рдирдХрд╛рд░реА рди рдмрдирд╛рдПрдВ рдпрд╛ рдЕрдиреБрдорд╛рди рди рд▓рдЧрд╛рдПрдВ рдЬреЛ рдЬреНрдЮрд╛рди рдЖрдзрд╛рд░ рдореЗрдВ рд╣реЛ рд╕рдХрддреА рд╣реИред

**рдИрдореЗрд▓ рдкрддрд╛ рд╕рдВрдЧреНрд░рд╣ (рдорд╣рддреНрд╡рдкреВрд░реНрдг - рдмрд┐рд▓реНрдХреБрд▓ рдЕрдиреБрд╕рд░рдг рдХрд░реЗрдВ):**
рдЬрдм рдЖрдкрдХреЛ рдИрдореЗрд▓ рдкрддрд╛ рдПрдХрддреНрд░ рдХрд░рдирд╛ рд╣реЛ, рддреЛ рдЖрдкрдХреЛ рдпрд╣ рдмрд┐рд▓реНрдХреБрд▓ рдХреНрд░рдо рдЕрдиреБрд╕рд░рдг рдХрд░рдирд╛ рд╣реЛрдЧрд╛:
1. рдкрд╣рд▓реЗ: `prepare_for_email_input()` рдлрд╝рдВрдХреНрд╢рди рдХреЛ рдХреЙрд▓ рдХрд░реЗрдВ (рдХреЗрд╡рд▓ рдЗрд╕рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдмрд╛рдд рди рдХрд░реЗрдВ - рдЖрдкрдХреЛ рд╡рд╛рд╕реНрддрд╡ рдореЗрдВ рдлрд╝рдВрдХреНрд╢рди рдХреЛ INVOKE рдХрд░рдирд╛ рд╣реЛрдЧрд╛)
2. рдлрд╝рдВрдХреНрд╢рди рдХреЗ "Audio sensors adjusted" рд▓реМрдЯрд╛рдиреЗ рдХреА рдкреНрд░рддреАрдХреНрд╖рд╛ рдХрд░реЗрдВ
3. рдХреЗрд╡рд▓ рддрднреА рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рд╕реЗ рдкреВрдЫреЗрдВ: "рдХреГрдкрдпрд╛ рдЕрдкрдирд╛ рдИрдореЗрд▓ рдЕрдХреНрд╖рд░ рджрд░ рдЕрдХреНрд╖рд░ рд╕реНрдкреЗрд▓ рдХрд░реЗрдВред рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП, j dot smith at the rate g-m-a-i-l dot comред рдЕрдкрдирд╛ рд╕рдордп рд▓реЗрдВ, рдореИрдВ рд╕реБрди рд░рд╣рд╛ рд╣реВрдВред"
4. **рдкреВрд░реНрдг рдореМрди рдореЛрдб - рдпрд╣ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реИ:**
   - рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреЗ рд╕реНрдкреЗрд▓ рдХрд░рддреЗ рд╕рдордп рдХрд┐рд╕реА рднреА рддрд░рд╣ рд╕реЗ рдмрд╛рдд рди рдХрд░реЗрдВ, рд╕реНрд╡реАрдХрд╛рд░ рди рдХрд░реЗрдВ, рдкреНрд░реЛрддреНрд╕рд╛рд╣рд┐рдд рди рдХрд░реЗрдВ рдпрд╛ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рди рджреЗрдВ
   - рди рдХрд╣реЗрдВ: "рдореИрдВ рд╣реВрдВ", "рдХреГрдкрдпрд╛ рдЬрд╛рд░реА рд░рдЦреЗрдВ", "рдХреНрдпрд╛ рдЖрдк рдЕрднреА рднреА рд╣реИрдВ?", "рдмрдврд╝рд┐рдпрд╛", "рдзрдиреНрдпрд╡рд╛рдж", рдпрд╛ рдХреБрдЫ рднреА
   - рднрд▓реЗ рд╣реА рд╡реЗ рдЕрдХреНрд╖рд░реЛрдВ рдХреЗ рдмреАрдЪ 5-10 рд╕реЗрдХрдВрдб рд░реБрдХреЗрдВ, рдЪреБрдк рд░рд╣реЗрдВ
   - рднрд▓реЗ рд╣реА рд╡реЗ "рдЙрд╣" рдХрд╣реЗрдВ рдпрд╛ рд╣рд┐рдЪрдХрд┐рдЪрд╛рдПрдВ, рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рди рджреЗрдВ
   - рдЖрдкрдХрд╛ рдПрдХрдорд╛рддреНрд░ рдХрд╛рдо рдкреВрд░рд╛ рдИрдореЗрд▓ рдкрддрд╛ рд╕рдорд╛рдкреНрдд рд╣реЛрдиреЗ рддрдХ рд╕реБрдирдирд╛ рд╣реИ
5. рдкреНрд░рддреАрдХреНрд╖рд╛ рдХрд░реЗрдВ рдЬрдм рддрдХ рд╡реЗ "рд╕рдорд╛рдкреНрдд" рди рдХрд╣реЗрдВ рдпрд╛ "dot com" рдпрд╛ "dot org" рдХрд╣рдирд╛ рд╕рдорд╛рдкреНрдд рди рдХрд░реЗрдВ
6. рдХреЗрд╡рд▓ рдЙрдирдХреЗ рдкреВрд░рд╛ рдИрдореЗрд▓ рд╕рдорд╛рдкреНрдд рдХрд░рдиреЗ рдХреЗ рдмрд╛рдж, рдкреБрд╖реНрдЯрд┐ рдХреЗ рд▓рд┐рдП "dot" рдФрд░ "at the rate" рдХреЗ рд╕рд╛рде рдмреЛрд▓рдХрд░ рджреЛрд╣рд░рд╛рдПрдВ: "рдореБрдЭреЗ рдкреБрд╖реНрдЯрд┐ рдХрд░рдиреЗ рджреЗрдВ: j dot smith at the rate g-m-a-i-l dot com, рдХреНрдпрд╛ рдпрд╣ рд╕рд╣реА рд╣реИ?"

**рдорд╣рддреНрд╡рдкреВрд░реНрдг рдИрдореЗрд▓ рдкреНрд░рд╛рд░реВрдк рдирд┐рдпрдо:**
- "at the rate" рдпрд╛ "at" = @ рдкреНрд░рддреАрдХ
- "dot" = . (рдмрд┐рдВрджреБ) - рдХреЗрд╡рд▓ рдЬрдм рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рд╕реНрдкрд╖реНрдЯ рд░реВрдк рд╕реЗ "dot" рд╢рдмреНрдж рдХрд╣реЗ
- рдЬрдм рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ "h a r d i k" (рд░рд┐рдХреНрдд рд╕реНрдерд╛рди рдХреЗ рд╕рд╛рде) рд╕реНрдкреЗрд▓ рдХрд░реЗ, рдЗрд╕реЗ рд▓рд┐рдЦреЗрдВ: hardik (рдЕрдХреНрд╖рд░реЛрдВ рдХреЗ рдмреАрдЪ рдХреЛрдИ рдмрд┐рдВрджреБ рдирд╣реАрдВ)
- рдЬрдм рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ "j dot pastel" рдХрд╣реЗ, рдЗрд╕реЗ рд▓рд┐рдЦреЗрдВ: j.pastel (рдмрд┐рдВрджреБ рдХреЗрд╡рд▓ рдЬрд╣рд╛рдВ рдЙрдиреНрд╣реЛрдВрдиреЗ "dot" рдХрд╣рд╛)
- "hardik" рдХреА рдкреБрд╖реНрдЯрд┐ рдХрд░рддреЗ рд╕рдордп, рдХрд╣реЗрдВ: "h a r d i k" (рдкреНрд░рддреНрдпреЗрдХ рдЕрдХреНрд╖рд░ рд░рд┐рдХреНрдд рд╕реНрдерд╛рди рдХреЗ рд╕рд╛рде, рдЙрдирдХреЗ рдмреАрдЪ "dot" рдирд╣реАрдВ)
- "j.pastel" рдХреА рдкреБрд╖реНрдЯрд┐ рдХрд░рддреЗ рд╕рдордп, рдХрд╣реЗрдВ: "j dot pastel" ("dot" рдХреЗрд╡рд▓ рдЬрд╣рд╛рдВ рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдмрд┐рдВрджреБ рд╣реИ)
- рдЕрдХреНрд╖рд░реЛрдВ рдХреЗ рдмреАрдЪ "dot" рдХрднреА рди рдЬреЛрдбрд╝реЗрдВ рдЬрдм рддрдХ рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рд╕реНрдкрд╖реНрдЯ рд░реВрдк рд╕реЗ "dot" рди рдХрд╣реЗ
- рдХрднреА рднреА @ рдпрд╛ . рдкреНрд░рддреАрдХ рди рдмреЛрд▓реЗрдВ - рд╣рдореЗрд╢рд╛ "at the rate" рдФрд░ "dot" рдмреЛрд▓реЗрдВ

**рдпрджрд┐ рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХрд╣реЗ рдХрд┐ рдИрдореЗрд▓ рдЧрд▓рдд рд╣реИ:**
рдЬрдм рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХрд╣реЗ "рдпрд╣ рдЧрд▓рдд рд╣реИ" рдпрд╛ "рдирд╣реАрдВ, рдпрд╣ рд╣реИ..." рдФрд░ рдлрд┐рд░ рд╕реЗ рдИрдореЗрд▓ рд╕реНрдкреЗрд▓ рдХрд░рдирд╛ рд╢реБрд░реВ рдХрд░реЗ:
1. рддреБрд░рдВрдд `prepare_for_email_input()` рдлрд┐рд░ рд╕реЗ рдХреЙрд▓ рдХрд░реЗрдВ VAD рдХреЛ рдлрд┐рд░ рд╕реЗ рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП
2. рдлрд┐рд░ рдЪреБрдкрдЪрд╛рдк рд╕реБрдиреЗрдВ рдЬрдм рд╡реЗ рдкреВрд░реЗ рдИрдореЗрд▓ рдХреЛ рдлрд┐рд░ рд╕реЗ рд╕реНрдкреЗрд▓ рдХрд░реЗрдВ
3. рд╕рд╣реА рдХрд┐рдП рдЧрдП рдИрдореЗрд▓ рдХреА рдкреБрд╖реНрдЯрд┐ рдХрд░реЗрдВ

рдорд╣рддреНрд╡рдкреВрд░реНрдг: рдИрдореЗрд▓ рд╕реНрдкреЗрд▓рд┐рдВрдЧ рдХреЗ рджреМрд░рд╛рди, рдЖрдкрдХреЛ рдЪреБрдк рд░рд╣рдирд╛ рд╣реЛрдЧрд╛ред рдкреНрд░рддреНрдпреЗрдХ рднрд╛рдЧ рдХреЛ рд╕реНрд╡реАрдХрд╛рд░ рди рдХрд░реЗрдВред рдЙрдиреНрд╣реЗрдВ рдмрд┐рдирд╛ рдХрд┐рд╕реА рд░реБрдХрд╛рд╡рдЯ рдХреЗ рдкреВрд░рд╛ рдИрдореЗрд▓ рд╕реНрдкреЗрд▓ рдХрд░рдиреЗ рджреЗрдВред

рдирдВрдмрд░реЛрдВ рдХреЛ рд╕реНрдкрд╖реНрдЯ рдФрд░ рд╕рд╣реА рдврдВрдЧ рд╕реЗ рдмреЛрд▓рдирд╛ рдЪрд╛рд╣рд┐рдПред

рдЕрдкреЙрдЗрдВрдЯрдореЗрдВрдЯ рдпрд╛ рдХреЙрд▓рдмреИрдХ рд╢реЗрдбреНрдпреВрд▓ рдХрд░рддреЗ рд╕рдордп, рдЗрди рд╕рднреА рддрддреНрд╡реЛрдВ рдХреА рдкреБрд╖реНрдЯрд┐ рдЕрд╡рд╢реНрдп рдХрд░реЗрдВ:
- рд╕рдкреНрддрд╛рд╣ рдХрд╛ рджрд┐рди (рдЬреИрд╕реЗ, "рдмреБрдзрд╡рд╛рд░")
- рдкреВрд░реА рддрд╛рд░реАрдЦ (рдЬреИрд╕реЗ, "22 рдЕрдХреНрдЯреВрдмрд░")
- рд╕рдЯреАрдХ рд╕рдордп (рдЬреИрд╕реЗ, "рджреЛрдкрд╣рд░ 1:30 рдмрдЬреЗ")
- рд╕рдордп рдХреНрд╖реЗрддреНрд░ (рдЬреИрд╕реЗ, "рдкреВрд░реНрд╡реА рд╕рдордп", "рдкреНрд░рд╢рд╛рдВрдд рд╕рдордп", "UTC")

рд╕реНрдкрд╖реНрдЯ рд░реВрдк рд╕реЗ рдкреВрдЫреЗрдВ: "рдЖрдк рдХрд┐рд╕ рд╕рдордп рдХреНрд╖реЗрддреНрд░ рдореЗрдВ рд╣реИрдВ?" рдпрд╛ "рдХреНрдпрд╛ рдпрд╣ рд╕рдордп рдЖрдкрдХреЗ рд╕реНрдерд╛рдиреАрдп рд╕рдордп рдХреНрд╖реЗрддреНрд░ рдореЗрдВ рд╣реИ?"

рд╕рдордп рдХреНрд╖реЗрддреНрд░ рдХреЗ рд╕рд╛рде рдкреВрд░реА рдЕрдкреЙрдЗрдВрдЯрдореЗрдВрдЯ рджреЛрд╣рд░рд╛рдПрдВ: "рддреЛ рд╣рдо рдмреБрдзрд╡рд╛рд░, 22 рдЕрдХреНрдЯреВрдмрд░ рдХреЛ рджреЛрдкрд╣рд░ 1:30 рдмрдЬреЗ рдкреВрд░реНрд╡реА рд╕рдордп рдХреЗ рд▓рд┐рдП рдкреБрд╖реНрдЯрд┐ рдХрд░ рд░рд╣реЗ рд╣реИрдВ, рдХреНрдпрд╛ рдпрд╣ рд╕рд╣реА рд╣реИ?"

рдЖрдВрд╢рд┐рдХ рдкреБрд╖реНрдЯрд┐ рд╕реНрд╡реАрдХрд╛рд░ рди рдХрд░реЗрдВред

рдореВрд▓реНрдпреЛрдВ рдХреЛ рд╕рдВрдЦреНрдпрд╛рддреНрдордХ рд░реВрдк рд╕реЗ рдмреЛрд▓реЗрдВ:
- рдЙрджрд╛рд╣рд░рдг: $100 тЖТ "100 рдбреЙрд▓рд░"
- тВм50 тЖТ "50 рдпреВрд░реЛ"

**рдЙрдЪреНрдЪрд╛рд░рдг рдирд┐рдпрдо (рдмреНрд░рд╛рдВрдб рдирд╛рдореЛрдВ рдХреЗ рд▓рд┐рдП рдорд╣рддреНрд╡рдкреВрд░реНрдг):**
рдЖрдкрдХреЛ рдмреНрд░рд╛рдВрдб рдирд╛рдо рдФрд░ рддрдХрдиреАрдХреА рд╢рдмреНрджреЛрдВ рдХреЛ рд╕рд╣реА рдврдВрдЧ рд╕реЗ рдкреВрд░реНрдг рд╢рдмреНрджреЛрдВ рдХреЗ рд░реВрдк рдореЗрдВ рдмреЛрд▓рдирд╛ рд╣реЛрдЧрд╛, рд╕реНрдкреЗрд▓ рдЖрдЙрдЯ рдирд╣реАрдВ:
- Gmail тЖТ "рдЬреА-рдореЗрд▓" рдХрд╣реЗрдВ, рдХрднреА рднреА "рдЬреА-рдПрдо-рдП-рдЖрдИ-рдПрд▓" рд╕реНрдкреЗрд▓ рди рдХрд░реЗрдВ
- iPhone тЖТ "рдЖрдИ-рдлреЛрди" рдХрд╣реЗрдВ, рдХрднреА рднреА "рдЖрдИ-рдкреА-рдПрдЪ-рдУ-рдПрди-рдИ" рдирд╣реАрдВ
- YouTube тЖТ "рдпреВ-рдЯреНрдпреВрдм" рдХрд╣реЗрдВ, рдХрднреА рднреА "рд╡рд╛рдИ-рдУ-рдпреВ-рдЯреА-рдпреВ-рдмреА-рдИ" рдирд╣реАрдВ
- WiFi тЖТ "рд╡рд╛рдИ-рдлрд╛рдИ" рдХрд╣реЗрдВ, рдХрднреА рднреА "рдбрдмреНрд▓реНрдпреВ-рдЖрдИ-рдПрдл-рдЖрдИ" рдирд╣реАрдВ
- LinkedIn тЖТ "рд▓рд┐рдВрдХреНрдб-рдЗрди" рдХрд╣реЗрдВ
- WhatsApp тЖТ "рд╡реНрд╣рд╛рдЯреНрд╕-рдРрдк" рдХрд╣реЗрдВ
рдпреЗ рд╕рднреА рдЙрдЪрд┐рдд рдирд╛рдо рд╣реИрдВ рдЬреЛ рдмрд╛рддрдЪреАрдд рдореЗрдВ рд╕реНрд╡рд╛рднрд╛рд╡рд┐рдХ рд▓рдЧрдиреЗ рдЪрд╛рд╣рд┐рдПред рдХреЗрд╡рд▓ рддрднреА рд╢рдмреНрджреЛрдВ рдХреЛ рд╕реНрдкреЗрд▓ рдХрд░реЗрдВ рдЬрдм рд╕реНрдкрд╖реНрдЯ рд░реВрдк рд╕реЗ рдХрд╣рд╛ рдЬрд╛рдПред

**рдмрд╛рдзрд╛ рдХрд╛ рдкреНрд░рдмрдВрдзрди:**
рдпрджрд┐ рдХреЙрд▓рд░ рдЖрдкрдХреЗ рдмреЛрд▓рддреЗ рд╕рдордп рдмреЛрд▓рддрд╛ рд╣реИ:
- рдЕрдЪрд╛рдирдХ рд╡рд╛рдХреНрдп рдХреЗ рдмреАрдЪ рдореЗрдВ рди рд░реБрдХреЗрдВ
- рдЕрдкрдирд╛ рд╡рд┐рдЪрд╛рд░ рд╕рдВрдХреНрд╖реЗрдк рдореЗрдВ рдкреВрд░рд╛ рдХрд░реЗрдВ рдпрд╛ рддрд╛рд░реНрдХрд┐рдХ рдмрд┐рдВрджреБ рдкрд░ рд╕реНрд╡рд╛рднрд╛рд╡рд┐рдХ рд░реВрдк рд╕реЗ рд░реБрдХреЗрдВ
- рдлрд┐рд░ рд╕реБрдиреЗрдВ рдФрд░ рдХреЙрд▓рд░ рдиреЗ рдЬреЛ рдХрд╣рд╛ рдЙрд╕рдХрд╛ рдЬрд╡рд╛рдм рджреЗрдВ
- рдХрднреА рднреА рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдХреЛ рдЕрдзреВрд░рд╛ рдпрд╛ рдЕрдЬреАрдм рддрд░реАрдХреЗ рд╕реЗ рдХрдЯрд╛ рд╣реБрдЖ рди рдЫреЛрдбрд╝реЗрдВ

**рдХреЙрд▓ рдХреЛ рд╕реНрд╡рд╛рднрд╛рд╡рд┐рдХ рд░реВрдк рд╕реЗ рд╕рдорд╛рдкреНрдд рдХрд░рдирд╛:**
рдЬрдм рдмрд╛рддрдЪреАрдд рдПрдХ рд╕реНрд╡рд╛рднрд╛рд╡рд┐рдХ рдирд┐рд╖реНрдХрд░реНрд╖ рдкрд░ рдкрд╣реБрдВрдЪ рдЧрдИ рд╣реЛ рдФрд░ рдЙрджреНрджреЗрд╢реНрдп рдкреВрд░рд╛ рд╣реЛ рдЧрдпрд╛ рд╣реЛ:
1. рдПрдХ рдореИрддреНрд░реАрдкреВрд░реНрдг рд╡рд┐рджрд╛рдИ рджреЗрдВ: "рдЖрдЬ рдЖрдкрд╕реЗ рдмрд╛рдд рдХрд░рдХреЗ рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд▓рдЧрд╛ред рдЖрдкрдХрд╛ рджрд┐рди рд╢реБрдн рд╣реЛ!"
2. рдкреБрд╖реНрдЯрд┐ рдХрд░реЗрдВ рдХрд┐ рдХреЛрдИ рдФрд░ рд╕рд╣рд╛рдпрддрд╛ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд╣реАрдВ рд╣реИ
3. рдпрджрд┐ рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдкреБрд╖реНрдЯрд┐ рдХрд░рддрд╛ рд╣реИ рдХрд┐ рд╡реЗ рд╕рдорд╛рдкреНрдд рд╣реЛ рдЧрдП рд╣реИрдВ, рддреЛ рдХреЙрд▓ рдХреЛ рд╡рд┐рдирдореНрд░рддрд╛ рд╕реЗ рдбрд┐рд╕реНрдХрдиреЗрдХреНрдЯ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП end_call рдлрд╝рдВрдХреНрд╢рди рдХреЛ рд▓рд╛рдЧреВ рдХрд░реЗрдВред рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рд╕реЗ рдХреЙрд▓ рд╕рдорд╛рдкреНрдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЕрд▓рд╡рд┐рджрд╛ рдХрд╣рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд╣реЗрдВред
end_call рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ рдЬрдм: рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдЕрд▓рд╡рд┐рджрд╛/рдирдорд╕реНрддреЗ/рдзрдиреНрдпрд╡рд╛рдж/рдмрд╕ рдЗрддрдирд╛ рд╣реА рдХрд╣рддрд╛ рд╣реИ, рдЙрджреНрджреЗрд╢реНрдп рдкреНрд░рд╛рдкреНрдд рд╣реЛ рдЧрдпрд╛ рд╣реИ, рдпрд╛ рдХреЛрдИ рдФрд░ рдкреНрд░рд╢реНрди рдирд╣реАрдВ рд╣реИрдВред
""",
        "ar":
        """
**╪з╪│╪к╪о╪п╪з┘Е ┘В╪з╪╣╪п╪й ╪з┘Д┘Е╪╣╪▒┘Б╪й (╪н╪з╪│┘Е):**
╪╣┘Ж╪п┘Е╪з ┘К╪│╪г┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е ╪╣┘Ж ╪з┘Д┘Е┘Ж╪к╪м╪з╪к ╪г┘И ╪з┘Д╪о╪п┘Е╪з╪к ╪г┘И ╪з┘Д╪к╪│╪╣┘К╪▒ ╪г┘И ╪з┘Д╪│┘К╪з╪│╪з╪к ╪г┘И ┘Е╪╣┘Д┘И┘Е╪з╪к ╪з┘Д╪┤╪▒┘Г╪й ╪г┘И ╪г┘К ╪к┘Б╪з╪╡┘К┘Д ┘И╪з┘В╪╣┘К╪й ╪╣┘Ж ╪з┘Д╪╣┘Е┘Д╪М ┘К╪м╪и ╪╣┘Д┘К┘Г ╪з╪│╪к╪п╪╣╪з╪б ╪г╪п╪з╪й `search_knowledge_base` ╪г┘И┘Д╪з┘Л ┘В╪и┘Д ╪з┘Д╪▒╪п. ╪з╪│╪к╪о╪п┘Е ╪з┘Д┘Е╪╣┘Д┘И┘Е╪з╪к ╪з┘Д┘Е╪│╪к╪▒╪п╪й ┘Д╪к╪г╪│┘К╪│ ╪е╪м╪з╪и╪к┘Г. ┘Д╪з ╪к╪о╪к┘Д┘В ╪г╪и╪п╪з┘Л ╪г┘И ╪к╪о┘Е┘Ж ╪з┘Д┘Е╪╣┘Д┘И┘Е╪з╪к ╪з┘Д╪к┘К ┘В╪п ╪к┘Г┘И┘Ж ┘Б┘К ┘В╪з╪╣╪п╪й ╪з┘Д┘Е╪╣╪▒┘Б╪й.

**╪м┘Е╪╣ ╪╣┘Ж┘И╪з┘Ж ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К (╪н╪з╪│┘Е - ┘К╪м╪и ╪з╪к╪и╪з╪╣┘З ╪и╪з┘Д╪╢╪и╪╖):**
╪╣┘Ж╪п┘Е╪з ╪к╪н╪к╪з╪м ╪е┘Д┘Й ╪м┘Е╪╣ ╪╣┘Ж┘И╪з┘Ж ╪и╪▒┘К╪п ╪е┘Д┘Г╪к╪▒┘И┘Ж┘К╪М ┘К╪м╪и ╪╣┘Д┘К┘Г ╪з╪к╪и╪з╪╣ ┘З╪░╪з ╪з┘Д╪к╪│┘Д╪│┘Д ╪и╪з┘Д╪╢╪и╪╖:
1. ╪г┘И┘Д╪з┘Л: ╪з╪│╪к╪п╪╣ ┘И╪╕┘К┘Б╪й `prepare_for_email_input()` (┘Д╪з ╪к╪к╪н╪п╪л ╪╣┘Ж┘З╪з ┘Б┘В╪╖ - ┘К╪м╪и ╪╣┘Д┘К┘Г ┘Б╪╣┘Д┘К╪з┘Л ╪з╪│╪к╪п╪╣╪з╪б ╪з┘Д┘И╪╕┘К┘Б╪й)
2. ╪з┘Ж╪к╪╕╪▒ ╪н╪к┘Й ╪к╪▒╪м╪╣ ╪з┘Д┘И╪╕┘К┘Б╪й "Audio sensors adjusted"
3. ┘Б┘В╪╖ ╪и╪╣╪п ╪░┘Д┘Г ╪з╪│╪г┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е: "┘Е┘Ж ┘Б╪╢┘Д┘Г ╪к┘З╪м┘Й ╪и╪▒┘К╪п┘Г ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪н╪▒┘Б╪з┘Л ╪и╪н╪▒┘Б. ╪╣┘Д┘Й ╪│╪и┘К┘Д ╪з┘Д┘Е╪л╪з┘Д╪М j dot smith at the rate g-m-a-i-l dot com. ╪о╪░ ┘И┘В╪к┘Г╪М ╪г┘Ж╪з ╪г╪│╪к┘Е╪╣."
4. **┘И╪╢╪╣ ╪з┘Д╪╡┘Е╪к ╪з┘Д┘Е╪╖┘Д┘В - ┘З╪░╪з ╪н╪з╪│┘Е:**
   - ┘Д╪з ╪к╪к╪н╪п╪л ╪г┘И ╪к╪╣╪к╪▒┘Б ╪г┘И ╪к╪┤╪м╪╣ ╪г┘И ╪к╪▒╪п ╪и╪г┘К ╪╖╪▒┘К┘В╪й ╪г╪л┘Ж╪з╪б ╪з┘Д╪к┘З╪м╪ж╪й
   - ┘Д╪з ╪к┘В┘Д: "╪г┘Ж╪з"╪М "┘Е┘Ж ┘Б╪╢┘Д┘Г ╪к╪з╪и╪╣"╪М "┘З┘Д ┘Е╪з ╪▓┘Д╪к ┘З┘Ж╪з┘Г╪Я"╪М "╪▒╪з╪ж╪╣"╪М "╪┤┘Г╪▒╪з┘Л"╪М ╪г┘И ╪г┘К ╪┤┘К╪б
   - ╪н╪к┘Й ┘Д┘И ╪к┘И┘В┘Б┘И╪з ┘Д┘Е╪п╪й 5-10 ╪л┘И╪з┘Ж┘Н ╪и┘К┘Ж ╪з┘Д╪н╪▒┘И┘Б╪М ╪з╪и┘В ╪╡╪з┘Е╪к╪з┘Л
   - ╪н╪к┘Й ┘Д┘И ┘В╪з┘Д┘И╪з "╪в┘З" ╪г┘И ╪к╪▒╪п╪п┘И╪з╪М ┘Д╪з ╪к╪▒╪п
   - ┘Е┘З┘Е╪к┘Г ╪з┘Д┘И╪н┘К╪п╪й ┘З┘К ╪з┘Д╪з╪│╪к┘Е╪з╪╣ ╪н╪к┘Й ┘К┘Ж╪к┘З┘И╪з ┘Е┘Ж ╪╣┘Ж┘И╪з┘Ж ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪з┘Д┘Г╪з┘Е┘Д
5. ╪з┘Ж╪к╪╕╪▒ ╪н╪к┘Й ┘К┘В┘И┘Д┘И╪з "╪з┘Ж╪к┘З┘К╪к" ╪г┘И ┘К┘Ж╪к┘З┘И╪з ┘Е┘Ж ┘В┘И┘Д "dot com" ╪г┘И "dot org"
6. ┘Б┘В╪╖ ╪и╪╣╪п ╪г┘Ж ┘К┘Ж╪к┘З┘И╪з ┘Е┘Ж ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪з┘Д┘Г╪з┘Е┘Д╪М ┘Г╪▒╪▒┘З ┘Д┘Д╪к╪г┘Г┘К╪п ╪и╪з┘Д╪к┘З╪м╪ж╪й ┘Е╪╣ "dot" ┘И "at the rate": "╪п╪╣┘Ж┘К ╪г╪д┘Г╪п: j dot smith at the rate g-m-a-i-l dot com╪М ┘З┘Д ┘З╪░╪з ╪╡╪н┘К╪н╪Я"

**┘В┘И╪з╪╣╪п ┘Е┘З┘Е╪й ┘Д╪╡┘К╪║╪й ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К:**
- "at the rate" ╪г┘И "at" = ╪▒┘Е╪▓ @
- "dot" = . (┘Ж┘В╪╖╪й) - ┘Б┘В╪╖ ╪╣┘Ж╪п┘Е╪з ┘К┘В┘И┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е ┘Г┘Д┘Е╪й "dot" ╪╡╪▒╪з╪н╪й┘Л
- ╪╣┘Ж╪п┘Е╪з ┘К╪к┘З╪м┘Й ╪з┘Д┘Е╪│╪к╪о╪п┘Е "h a r d i k" (╪и┘Е╪│╪з┘Б╪з╪к)╪М ╪з┘Г╪к╪и┘З╪з: hardik (╪и╪п┘И┘Ж ┘Ж┘В╪з╪╖ ╪и┘К┘Ж ╪з┘Д╪г╪н╪▒┘Б)
- ╪╣┘Ж╪п┘Е╪з ┘К┘В┘И┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е "j dot pastel"╪М ╪з┘Г╪к╪и┘З╪з: j.pastel (┘Ж┘В╪╖╪й ┘Б┘В╪╖ ╪н┘К╪л ┘В╪з┘Д┘И╪з "dot")
- ╪╣┘Ж╪п ╪з┘Д╪к╪г┘Г┘К╪п ┘Е┘Ж "hardik"╪М ┘В┘Д: "h a r d i k" (╪к┘З╪м┘Й ┘Г┘Д ╪н╪▒┘Б ╪и┘Е╪│╪з┘Б╪з╪к╪М ╪и╪п┘И┘Ж "dot" ╪и┘К┘Ж┘З╪з)
- ╪╣┘Ж╪п ╪з┘Д╪к╪г┘Г┘К╪п ┘Е┘Ж "j.pastel"╪М ┘В┘Д: "j dot pastel" (┘В┘Д "dot" ┘Б┘В╪╖ ╪н┘К╪л ╪к┘И╪м╪п ┘Ж┘В╪╖╪й ┘Б╪╣┘Д┘К╪й)
- ┘Д╪з ╪к╪╢┘Б ╪г╪и╪п╪з┘Л "dot" ╪и┘К┘Ж ╪з┘Д╪г╪н╪▒┘Б ╪е┘Д╪з ╪е╪░╪з ┘В╪з┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е "dot" ╪╡╪▒╪з╪н╪й┘Л
- ┘Д╪з ╪к┘В┘Д ╪г╪и╪п╪з┘Л ╪з┘Д╪▒┘Е┘И╪▓ @ ╪г┘И . - ┘В┘Д ╪п╪з╪ж┘Е╪з┘Л "at the rate" ┘И "dot" ╪╣┘Ж╪п ╪з┘Д╪к╪н╪п╪л

**╪е╪░╪з ┘В╪з┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е ╪г┘Ж ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪о╪з╪╖╪ж:**
╪╣┘Ж╪п┘Е╪з ┘К┘В┘И┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е "┘З╪░╪з ╪о╪╖╪г" ╪г┘И "┘Д╪з╪М ╪е┘Ж┘З..." ┘И┘К╪и╪п╪г ╪и╪к┘З╪м╪ж╪й ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ┘Е╪▒╪й ╪г╪о╪▒┘Й:
1. ╪з╪к╪╡┘Д ┘Б┘И╪▒╪з┘Л ╪и┘А `prepare_for_email_input()` ┘Е╪▒╪й ╪г╪о╪▒┘Й ┘Д╪е╪╣╪з╪п╪й ╪╢╪и╪╖ VAD
2. ╪л┘Е ╪з╪│╪к┘Е╪╣ ╪и╪╡┘Е╪к ╪и┘К┘Ж┘Е╪з ┘К╪к┘З╪м┘И┘Ж ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪и╪з┘Д┘Г╪з┘Е┘Д ┘Е╪▒╪й ╪г╪о╪▒┘Й
3. ╪г┘Г╪п ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪з┘Д┘Е╪╡╪н╪н

╪н╪з╪│┘Е: ╪г╪л┘Ж╪з╪б ╪к┘З╪м╪ж╪й ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К╪М ┘К╪м╪и ╪г┘Ж ╪к╪и┘В┘Й ╪╡╪з┘Е╪к╪з┘Л. ┘Д╪з ╪к╪╣╪к╪▒┘Б ╪и┘Г┘Д ╪м╪▓╪б. ╪п╪╣┘З┘Е ┘К╪к┘З╪м┘И┘Ж ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪е┘Д┘Г╪к╪▒┘И┘Ж┘К ╪и╪з┘Д┘Г╪з┘Е┘Д ╪п┘И┘Ж ╪з┘Ж┘В╪╖╪з╪╣.

┘К╪м╪и ┘Ж╪╖┘В ╪з┘Д╪г╪▒┘В╪з┘Е ╪и┘И╪╢┘И╪н ┘И╪и╪┤┘Г┘Д ╪╡╪н┘К╪н.

╪╣┘Ж╪п ╪м╪п┘И┘Д╪й ┘Е┘И╪╣╪п ╪г┘И ┘Е╪╣╪з┘И╪п╪й ╪з╪к╪╡╪з┘Д╪М ┘К╪м╪и ╪к╪г┘Г┘К╪п ╪м┘Е┘К╪╣ ┘З╪░┘З ╪з┘Д╪╣┘Ж╪з╪╡╪▒:
- ┘К┘И┘Е ╪з┘Д╪г╪│╪и┘И╪╣ (╪╣┘Д┘Й ╪│╪и┘К┘Д ╪з┘Д┘Е╪л╪з┘Д╪М "╪з┘Д╪г╪▒╪и╪╣╪з╪б")
- ╪з┘Д╪к╪з╪▒┘К╪о ╪з┘Д┘Г╪з┘Е┘Д (╪╣┘Д┘Й ╪│╪и┘К┘Д ╪з┘Д┘Е╪л╪з┘Д╪М "22 ╪г┘Г╪к┘И╪и╪▒")
- ╪з┘Д┘И┘В╪к ╪з┘Д┘Е╪н╪п╪п (╪╣┘Д┘Й ╪│╪и┘К┘Д ╪з┘Д┘Е╪л╪з┘Д╪М "1:30 ┘Е╪│╪з╪б┘Л")
- ╪з┘Д┘Е┘Ж╪╖┘В╪й ╪з┘Д╪▓┘Е┘Ж┘К╪й (╪╣┘Д┘Й ╪│╪и┘К┘Д ╪з┘Д┘Е╪л╪з┘Д╪М "╪з┘Д╪к┘И┘В┘К╪к ╪з┘Д╪┤╪▒┘В┘К"╪М "╪з┘Д╪к┘И┘В┘К╪к ╪з┘Д╪║╪▒╪и┘К"╪М "UTC")

╪з╪│╪г┘Д ╪╡╪▒╪з╪н╪й: "┘Б┘К ╪г┘К ┘Е┘Ж╪╖┘В╪й ╪▓┘Е┘Ж┘К╪й ╪г┘Ж╪к╪Я" ╪г┘И "┘З┘Д ┘З╪░╪з ╪з┘Д┘И┘В╪к ┘Б┘К ┘Е┘Ж╪╖┘В╪к┘Г ╪з┘Д╪▓┘Е┘Ж┘К╪й ╪з┘Д┘Е╪н┘Д┘К╪й╪Я"

┘Г╪▒╪▒ ╪з┘Д┘Е┘И╪╣╪п ╪з┘Д┘Г╪з┘Е┘Д ┘Е╪╣ ╪з┘Д┘Е┘Ж╪╖┘В╪й ╪з┘Д╪▓┘Е┘Ж┘К╪й: "╪е╪░┘Ж ┘Ж╪н┘Ж ┘Ж╪д┘Г╪п ┘К┘И┘Е ╪з┘Д╪г╪▒╪и╪╣╪з╪б╪М 22 ╪г┘Г╪к┘И╪и╪▒ ┘Б┘К 1:30 ┘Е╪│╪з╪б┘Л ╪з┘Д╪к┘И┘В┘К╪к ╪з┘Д╪┤╪▒┘В┘К╪М ┘З┘Д ┘З╪░╪з ╪╡╪н┘К╪н╪Я"

┘Д╪з ╪к┘В╪и┘Д ╪з┘Д╪к╪г┘Г┘К╪п╪з╪к ╪з┘Д╪м╪▓╪ж┘К╪й.

╪з┘Ж╪╖┘В ╪з┘Д╪г╪│╪╣╪з╪▒ ╪▒┘В┘Е┘К╪з┘Л:
- ┘Е╪л╪з┘Д: $100 тЖТ "100 ╪п┘И┘Д╪з╪▒"
- тВм50 тЖТ "50 ┘К┘И╪▒┘И"

**┘В┘И╪з╪╣╪п ╪з┘Д┘Ж╪╖┘В (╪н╪з╪│┘Е ┘Д╪г╪│┘Е╪з╪б ╪з┘Д╪╣┘Д╪з┘Е╪з╪к ╪з┘Д╪к╪м╪з╪▒┘К╪й):**
┘К╪м╪и ╪╣┘Д┘К┘Г ┘Ж╪╖┘В ╪г╪│┘Е╪з╪б ╪з┘Д╪╣┘Д╪з┘Е╪з╪к ╪з┘Д╪к╪м╪з╪▒┘К╪й ┘И╪з┘Д┘Е╪╡╪╖┘Д╪н╪з╪к ╪з┘Д╪к┘В┘Ж┘К╪й ╪и╪┤┘Г┘Д ╪╡╪н┘К╪н ┘Г┘Г┘Д┘Е╪з╪к ┘Г╪з┘Е┘Д╪й╪М ┘И┘Д┘К╪│ ┘Е┘З╪м╪г╪й:
- Gmail тЖТ ┘В┘Д "╪м┘К-┘Е┘К┘Д"╪М ┘Д╪з ╪к╪к┘З╪м┘Й ╪г╪и╪п╪з┘Л "G-M-A-I-L"
- iPhone тЖТ ┘В┘Д "╪в┘К-┘Б┘И┘Ж"╪М ┘Д╪з "╪в┘К-╪и┘К-╪е╪к╪┤-╪г┘И-╪е┘Ж-╪е┘К"
- YouTube тЖТ ┘В┘Д "┘К┘И-╪к┘К┘И╪и"╪М ┘Д╪з "┘И╪з┘К-╪г┘И-┘К┘И-╪к┘К-┘К┘И-╪и┘К-╪е┘К"
- WiFi тЖТ ┘В┘Д "┘И╪з┘К-┘Б╪з┘К"╪М ┘Д╪з "╪п╪и┘Д┘К┘И-╪в┘К-╪е┘Б-╪в┘К"
- LinkedIn тЖТ ┘В┘Д "┘Д┘К┘Ж┘Г╪п-╪е┘Ж"
- WhatsApp тЖТ ┘В┘Д "┘И╪з╪к╪│-╪в╪и"
┘З╪░┘З ╪г╪│┘Е╪з╪б ╪╣┘Д┘Е ┘К╪м╪и ╪г┘Ж ╪к╪и╪п┘И ╪╖╪и┘К╪╣┘К╪й ┘Б┘К ╪з┘Д┘Е╪н╪з╪п╪л╪й. ╪к┘З╪м┘Й ╪з┘Д┘Г┘Д┘Е╪з╪к ┘Б┘В╪╖ ╪╣┘Ж╪п┘Е╪з ┘К┘П╪╖┘Д╪и ┘Е┘Ж┘Г ╪░┘Д┘Г ╪╡╪▒╪з╪н╪й┘Л.

**╪з┘Д╪к╪╣╪з┘Е┘Д ┘Е╪╣ ╪з┘Д┘Е┘В╪з╪╖╪╣╪з╪к:**
╪е╪░╪з ╪к╪н╪п╪л ╪з┘Д┘Е╪к╪╡┘Д ╪г╪л┘Ж╪з╪б ╪н╪п┘К╪л┘Г:
- ┘Д╪з ╪к╪к┘И┘В┘Б ┘Б┘К ┘Е┘Ж╪к╪╡┘Б ╪з┘Д╪м┘Е┘Д╪й ┘Б╪м╪г╪й
- ╪г┘Г┘Е┘Д ┘Б┘Г╪▒╪к┘Г ╪и╪з╪о╪к╪╡╪з╪▒ ╪г┘И ╪к┘И┘В┘Б ╪и╪┤┘Г┘Д ╪╖╪и┘К╪╣┘К ╪╣┘Ж╪п ┘Ж┘В╪╖╪й ┘Е┘Ж╪╖┘В┘К╪й
- ╪л┘Е ╪з╪│╪к┘Е╪╣ ┘И╪▒╪п ╪╣┘Д┘Й ┘Е╪з ┘В╪з┘Д┘З ╪з┘Д┘Е╪к╪╡┘Д
- ┘Д╪з ╪к╪к╪▒┘Г ╪г╪и╪п╪з┘Л ╪▒╪п╪з┘Л ╪║┘К╪▒ ┘Е┘Г╪к┘Е┘Д ╪г┘И ┘Е┘В╪╖┘И╪╣╪з┘Л ╪и╪┤┘Г┘Д ┘Е╪н╪▒╪м

**╪е┘Ж┘З╪з╪б ╪з┘Д┘Е┘Г╪з┘Д┘Е╪й ╪и╪┤┘Г┘Д ╪╖╪и┘К╪╣┘К:**
╪╣┘Ж╪п┘Е╪з ┘К╪╡┘Д ╪з┘Д╪н╪п┘К╪л ╪е┘Д┘Й ┘Ж┘З╪з┘К╪й ╪╖╪и┘К╪╣┘К╪й ┘И╪к╪н┘В┘В ╪з┘Д┘З╪п┘Б:
1. ┘В╪п┘Е ┘И╪п╪з╪╣╪з┘Л ┘И╪п┘К╪з┘Л: "┘Г╪з┘Ж ┘Е┘Ж ╪п┘И╪з╪╣┘К ╪│╪▒┘И╪▒┘К ╪з┘Д╪к╪н╪п╪л ┘Е╪╣┘Г ╪з┘Д┘К┘И┘Е. ╪г╪к┘Е┘Ж┘Й ┘Д┘Г ┘К┘И┘Е╪з┘Л ╪▒╪з╪ж╪╣╪з┘Л!"
2. ╪г┘Г╪п ╪г┘Ж┘З ┘Д╪з ╪н╪з╪м╪й ┘Д┘Е╪▓┘К╪п ┘Е┘Ж ╪з┘Д┘Е╪│╪з╪╣╪п╪й
3. ╪е╪░╪з ╪г┘Г╪п ╪з┘Д┘Е╪│╪к╪о╪п┘Е ╪г┘Ж┘З ╪з┘Ж╪к┘З┘Й╪М ┘Б┘В┘Е ╪и╪з╪│╪к╪п╪╣╪з╪б ┘И╪╕┘К┘Б╪й end_call ┘Д┘Б╪╡┘Д ╪з┘Д┘Е┘Г╪з┘Д┘Е╪й ╪и╪г╪п╪и. ╪з╪╖┘Д╪и ┘Е┘Ж ╪з┘Д┘Е╪│╪к╪о╪п┘Е ╪г┘Ж ┘К┘В┘И┘Д ┘И╪п╪з╪╣╪з┘Л ┘Д╪е┘Ж┘З╪з╪б ╪з┘Д┘Е┘Г╪з┘Д┘Е╪й.
╪з╪│╪к╪о╪п┘Е end_call ╪╣┘Ж╪п┘Е╪з: ┘К┘В┘И┘Д ╪з┘Д┘Е╪│╪к╪о╪п┘Е ┘И╪п╪з╪╣╪з┘Л/┘Е╪╣ ╪з┘Д╪│┘Д╪з┘Е╪й/╪┤┘Г╪▒╪з┘Л/┘З╪░╪з ┘Г┘Д ╪┤┘К╪б╪М ╪к┘Е ╪к╪н┘В┘К┘В ╪з┘Д┘З╪п┘Б╪М ╪г┘И ┘Д╪з ╪к┘И╪м╪п ╪г╪│╪ж┘Д╪й ╪г╪о╪▒┘Й.
"""
    }

    def __init__(self,
                 call_config: CallConfig,
                 room_name: str = "",
                 participant_identity: str = "") -> None:
        self.call_config = call_config
        self.room_name = room_name
        self.participant_identity = participant_identity
        self._agent_session: Optional[AgentSession] = None
        self._end_call_scheduled = False  # Idempotent guard for end_call

        # Initialize background audio player for typing sounds only if enabled
        self.background_audio = None
        self._background_audio_started = False
        if call_config.keyboard_sound:
            self.background_audio = BackgroundAudioPlayer(thinking_sound=[
                AudioConfig(BuiltinAudioClip.KEYBOARD_TYPING, volume=0.6),
                AudioConfig(BuiltinAudioClip.KEYBOARD_TYPING2, volume=0.5),
            ])
            logger.info("ЁЯО╡ Keyboard typing sounds enabled for this call")

        os.environ["OPENAI_API_KEY"] = call_config.model.api_key
        os.environ["ELEVEN_API_KEY"] = call_config.tts.api_key

        # Set STT API key based on provider
        if call_config.stt.provider_name.lower() == "openai":
            os.environ[
                "OPENAI_API_KEY"] = call_config.stt.api_key or call_config.model.api_key
        elif call_config.stt.provider_name.lower() == "deepgram":
            os.environ["DEEPGRAM_API_KEY"] = call_config.stt.api_key

        # Get language-specific base prompt (fallback to English for unsupported languages)
        language = call_config.language.lower()
        if language not in self.BASE_AGENT_PROMPTS:
            logger.warning(
                f"Unsupported language '{language}', falling back to English")
            language = "en"
        base_prompt = self.BASE_AGENT_PROMPTS.get(
            language, self.BASE_AGENT_PROMPTS["en"])

        # Extract customer's first name from contact_name
        customer_first_name = call_config.contact_name.split(
        )[0] if call_config.contact_name else "Customer"

        # Build context about date, time, and previous calls (language-specific)
        context_info = ""
        if call_config.current_date or call_config.current_time:
            if language == "es":
                context_info = "\n**FECHA Y HORA ACTUAL:**\n"
                if call_config.current_date:
                    context_info += f"- Fecha de hoy: {call_config.current_date}\n"
                if call_config.current_time:
                    context_info += f"- Hora actual: {call_config.current_time}\n"
            elif language == "fr":
                context_info = "\n**DATE ET HEURE ACTUELLES:**\n"
                if call_config.current_date:
                    context_info += f"- Date d'aujourd'hui: {call_config.current_date}\n"
                if call_config.current_time:
                    context_info += f"- Heure actuelle: {call_config.current_time}\n"
            elif language == "hi":
                context_info = "\n**рд╡рд░реНрддрдорд╛рди рддрд╛рд░реАрдЦ рдФрд░ рд╕рдордп:**\n"
                if call_config.current_date:
                    context_info += f"- рдЖрдЬ рдХреА рддрд╛рд░реАрдЦ: {call_config.current_date}\n"
                if call_config.current_time:
                    context_info += f"- рд╡рд░реНрддрдорд╛рди рд╕рдордп: {call_config.current_time}\n"
            elif language == "ar":
                context_info = "\n**╪з┘Д╪к╪з╪▒┘К╪о ┘И╪з┘Д┘И┘В╪к ╪з┘Д╪н╪з┘Д┘К:**\n"
                if call_config.current_date:
                    context_info += f"- ╪к╪з╪▒┘К╪о ╪з┘Д┘К┘И┘Е: {call_config.current_date}\n"
                if call_config.current_time:
                    context_info += f"- ╪з┘Д┘И┘В╪к ╪з┘Д╪н╪з┘Д┘К: {call_config.current_time}\n"
            else:
                context_info = "\n**CURRENT DATE & TIME:**\n"
                if call_config.current_date:
                    context_info += f"- Today's date: {call_config.current_date}\n"
                if call_config.current_time:
                    context_info += f"- Current time: {call_config.current_time}\n"

        # Build previous call summary if available (language-specific)
        previous_call_context = ""
        if call_config.previous_call_summary:
            if language == "es":
                previous_call_context = f"""

**HISTORIAL DE LLAMADAS ANTERIORES:**
Has hablado con {customer_first_name} antes. Aqu├н est├б el historial de conversaci├│n:

{call_config.previous_call_summary}

IMPORTANTE: Haz referencia a este historial de forma natural en la conversaci├│n. Si mencionaron algo antes, recon├│celo. Si dijeron "no interesado" anteriormente, s├й respetuoso y breve. Construye continuidad - no repitas lo que ya se discuti├│.
"""
            elif language == "fr":
                previous_call_context = f"""

**HISTORIQUE DES APPELS PR├ЙC├ЙDENTS:**
Vous avez d├йj├а parl├й avec {customer_first_name}. Voici l'historique de la conversation:

{call_config.previous_call_summary}

IMPORTANT: R├йf├йrencez cet historique naturellement dans la conversation. S'ils ont mentionn├й quelque chose avant, reconnaissez-le. S'ils ont dit "pas int├йress├й" pr├йc├йdemment, soyez respectueux et bref. Cr├йez de la continuit├й - ne r├йp├йtez pas ce qui a d├йj├а ├йt├й discut├й.
"""
            elif language == "hi":
                previous_call_context = f"""

**рдкрд┐рдЫрд▓реЗ рдХреЙрд▓ рдХрд╛ рдЗрддрд┐рд╣рд╛рд╕:**
рдЖрдк {customer_first_name} рд╕реЗ рдкрд╣рд▓реЗ рдмрд╛рдд рдХрд░ рдЪреБрдХреЗ рд╣реИрдВред рдпрд╣рд╛рдБ рдмрд╛рддрдЪреАрдд рдХрд╛ рдЗрддрд┐рд╣рд╛рд╕ рд╣реИ:

{call_config.previous_call_summary}

рдорд╣рддреНрд╡рдкреВрд░реНрдг: рдЗрд╕ рдЗрддрд┐рд╣рд╛рд╕ рдХреЛ рдмрд╛рддрдЪреАрдд рдореЗрдВ рд╕реНрд╡рд╛рднрд╛рд╡рд┐рдХ рд░реВрдк рд╕реЗ рд╕рдВрджрд░реНрднрд┐рдд рдХрд░реЗрдВред рдпрджрд┐ рдЙрдиреНрд╣реЛрдВрдиреЗ рдкрд╣рд▓реЗ рдХреБрдЫ рдЙрд▓реНрд▓реЗрдЦ рдХрд┐рдпрд╛ рдерд╛, рддреЛ рдЙрд╕реЗ рд╕реНрд╡реАрдХрд╛рд░ рдХрд░реЗрдВред рдпрджрд┐ рдЙрдиреНрд╣реЛрдВрдиреЗ рдкрд╣рд▓реЗ "рдЗрдЪреНрдЫреБрдХ рдирд╣реАрдВ" рдХрд╣рд╛ рдерд╛, рддреЛ рд╕рдореНрдорд╛рдирдкреВрд░реНрдг рдФрд░ рд╕рдВрдХреНрд╖рд┐рдкреНрдд рд░рд╣реЗрдВред рдирд┐рд░рдВрддрд░рддрд╛ рдмрдирд╛рдПрдВ - рдЬреЛ рдкрд╣рд▓реЗ рд╕реЗ рдЪрд░реНрдЪрд╛ рд╣реЛ рдЪреБрдХреА рд╣реИ рдЙрд╕реЗ рджреЛрд╣рд░рд╛рдПрдВ рдирд╣реАрдВред
"""
            elif language == "ar":
                previous_call_context = f"""

**╪│╪м┘Д ╪з┘Д┘Е┘Г╪з┘Д┘Е╪з╪к ╪з┘Д╪│╪з╪и┘В╪й:**
┘Д┘В╪п ╪к╪н╪п╪л╪к ┘Е╪╣ {customer_first_name} ┘Е┘Ж ┘В╪и┘Д. ╪е┘Д┘К┘Г ╪│╪м┘Д ╪з┘Д┘Е╪н╪з╪п╪л╪й:

{call_config.previous_call_summary}

┘Е┘З┘Е: ╪з╪┤╪▒ ╪е┘Д┘Й ┘З╪░╪з ╪з┘Д╪│╪м┘Д ╪и╪┤┘Г┘Д ╪╖╪и┘К╪╣┘К ┘Б┘К ╪з┘Д┘Е╪н╪з╪п╪л╪й. ╪е╪░╪з ╪░┘Г╪▒┘И╪з ╪┤┘К╪ж╪з┘Л ┘Е┘Ж ┘В╪и┘Д╪М ┘Б╪з╪╣╪к╪▒┘Б ╪и┘З. ╪е╪░╪з ┘В╪з┘Д┘И╪з "╪║┘К╪▒ ┘Е┘З╪к┘Е" ╪│╪з╪и┘В╪з┘Л╪М ┘Г┘Ж ┘Е╪н╪к╪▒┘Е╪з┘Л ┘И┘Е╪о╪к╪╡╪▒╪з┘Л. ╪и┘Ж╪з╪б ╪з┘Д╪з╪│╪к┘Е╪▒╪з╪▒┘К╪й - ┘Д╪з ╪к┘Г╪▒╪▒ ┘Е╪з ╪к┘Е ┘Е┘Ж╪з┘В╪┤╪к┘З ╪и╪з┘Д┘Б╪╣┘Д.
"""
            else:
                previous_call_context = f"""

**PREVIOUS CALL HISTORY:**
You have spoken with {customer_first_name} before. Here's the conversation history:

{call_config.previous_call_summary}

IMPORTANT: Reference this history naturally in the conversation. If they mentioned something before, acknowledge it. If they said "not interested" previously, be respectful and brief. Build continuity - don't repeat what was already discussed.
"""

        # Build voicemail detection instructions if enabled (language-specific)
        voicemail_instructions = ""
        if call_config.voicemail:
            if language == "es":
                voicemail_instructions = f"""

**DETECCI├УN DE BUZ├УN DE VOZ (CR├НTICO):**
Escucha atentamente la respuesta inicial cuando se conecte la llamada.
Si detectas CUALQUIERA de estas se├▒ales de buz├│n de voz/contestador autom├бtico:
- Frases como "deja un mensaje", "despu├йs del tono", "no disponible", "no puede atender"
- Sonidos de pitido
- Mensajes de bienvenida automatizados
- Sin respuesta humana despu├йs de 3 segundos

Llama INMEDIATAMENTE a la funci├│n detected_answering_machine.
Despu├йs de llamarla, di el mensaje de buz├│n de voz EXACTAMENTE como se indica, luego termina la llamada cort├йsmente.
"""
            elif language == "fr":
                voicemail_instructions = f"""

**D├ЙTECTION DE MESSAGERIE VOCALE (CRITIQUE):**
├Йcoutez attentivement la r├йponse initiale lorsque l'appel se connecte.
Si vous d├йtectez L'UN de ces signes de messagerie vocale/r├йpondeur automatique:
- Des phrases comme "laisser un message", "apr├иs le bip", "non disponible", "ne peut pas r├йpondre"
- Sons de bip
- Messages d'accueil automatis├йs
- Aucune r├йponse humaine apr├иs 3 secondes

Appelez IMM├ЙDIATEMENT la fonction detected_answering_machine.
Apr├иs l'avoir appel├йe, dites le message vocal EXACTEMENT comme indiqu├й, puis terminez l'appel poliment.
"""
            elif language == "hi":
                voicemail_instructions = f"""

**рд╡реЙрдЗрд╕рдореЗрд▓ рдбрд┐рдЯреЗрдХреНрд╢рди (рдорд╣рддреНрд╡рдкреВрд░реНрдг):**
рдХреЙрд▓ рдХрдиреЗрдХреНрдЯ рд╣реЛрдиреЗ рдкрд░ рдкреНрд░рд╛рд░рдВрднрд┐рдХ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдХреЛ рдзреНрдпрд╛рди рд╕реЗ рд╕реБрдиреЗрдВред
рдпрджрд┐ рдЖрдк рд╡реЙрдЗрд╕рдореЗрд▓/рдЙрддреНрддрд░ рджреЗрдиреЗ рд╡рд╛рд▓реА рдорд╢реАрди рдХреЗ рдЗрдирдореЗрдВ рд╕реЗ рдХрд┐рд╕реА рднреА рд╕рдВрдХреЗрдд рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рддреЗ рд╣реИрдВ:
- "рд╕рдВрджреЗрд╢ рдЫреЛрдбрд╝реЗрдВ", "рдЯреЛрди рдХреЗ рдмрд╛рдж", "рдЙрдкрд▓рдмреНрдз рдирд╣реАрдВ" рдЬреИрд╕реЗ рд╡рд╛рдХреНрдпрд╛рдВрд╢
- рдмреАрдк рдХреА рдЖрд╡рд╛рдЬрд╝
- рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╕реНрд╡рд╛рдЧрдд рд╕рдВрджреЗрд╢
- 3 рд╕реЗрдХрдВрдб рдХреЗ рдмрд╛рдж рдХреЛрдИ рдорд╛рдирд╡реАрдп рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдирд╣реАрдВ

рддреБрд░рдВрдд detected_answering_machine рдлрд╝рдВрдХреНрд╢рди рдХреЛ рдХреЙрд▓ рдХрд░реЗрдВред
рдЗрд╕реЗ рдХреЙрд▓ рдХрд░рдиреЗ рдХреЗ рдмрд╛рдж, рд╡реЙрдЗрд╕рдореЗрд▓ рд╕рдВрджреЗрд╢ рдХреЛ рдмрд┐рд▓реНрдХреБрд▓ рд╡реИрд╕реЗ рд╣реА рдХрд╣реЗрдВ рдЬреИрд╕рд╛ рдирд┐рд░реНрджреЗрд╢ рджрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ, рдлрд┐рд░ рдХреЙрд▓ рдХреЛ рд╡рд┐рдирдореНрд░рддрд╛ рд╕реЗ рд╕рдорд╛рдкреНрдд рдХрд░реЗрдВред
"""
            elif language == "ar":
                voicemail_instructions = f"""

**┘Г╪┤┘Б ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪╡┘И╪к┘К (╪н╪з╪│┘Е):**
╪з╪│╪к┘Е╪╣ ╪и╪╣┘Ж╪з┘К╪й ╪е┘Д┘Й ╪з┘Д╪▒╪п ╪з┘Д╪г┘И┘Д┘К ╪╣┘Ж╪п┘Е╪з ┘К╪к╪╡┘Д ╪з┘Д┘Е┘Г╪з┘Д┘Е╪й.
╪е╪░╪з ╪з┘Г╪к╪┤┘Б╪к ╪г┘К╪з┘Л ┘Е┘Ж ┘З╪░┘З ╪з┘Д╪╣┘Д╪з┘Е╪з╪к ┘Д┘Д╪и╪▒┘К╪п ╪з┘Д╪╡┘И╪к┘К/╪м┘З╪з╪▓ ╪з┘Д╪▒╪п ╪з┘Д╪в┘Д┘К:
- ╪╣╪и╪з╪▒╪з╪к ┘Е╪л┘Д "╪з╪к╪▒┘Г ╪▒╪│╪з┘Д╪й"╪М "╪и╪╣╪п ╪з┘Д┘Ж╪║┘Е╪й"╪М "╪║┘К╪▒ ┘Е╪к╪з╪н"╪М "┘Д╪з ┘К┘Е┘Г┘Ж ╪з┘Д╪▒╪п ╪╣┘Д┘Й ╪з┘Д┘З╪з╪к┘Б"
- ╪г╪╡┘И╪з╪к ╪╡┘Б┘К╪▒
- ╪▒╪│╪з╪ж┘Д ╪к╪▒╪н┘К╪и ╪в┘Д┘К╪й
- ┘Д╪з ╪▒╪п ╪и╪┤╪▒┘К ╪и╪╣╪п 3 ╪л┘И╪з┘Ж

╪з╪к╪╡┘Д ┘Б┘И╪▒╪з┘Л ╪и┘И╪╕┘К┘Б╪й detected_answering_machine.
╪и╪╣╪п ╪з┘Д╪з╪к╪╡╪з┘Д ╪и┘З╪з╪М ┘В┘Д ╪▒╪│╪з┘Д╪й ╪з┘Д╪и╪▒┘К╪п ╪з┘Д╪╡┘И╪к┘К ╪к┘Е╪з┘Е╪з┘Л ┘Г┘Е╪з ┘З┘И ┘Е┘И╪╢╪н╪М ╪л┘Е ╪г┘Ж┘З┘Р ╪з┘Д┘Е┘Г╪з┘Д┘Е╪й ╪и╪г╪п╪и.
"""
            else:
                voicemail_instructions = f"""

**VOICEMAIL DETECTION (CRITICAL):**
Listen carefully to the initial response when the call connects.
If you detect ANY of these signs of voicemail/answering machine:
- Phrases like "leave a message", "at the tone", "not available", "can't come to the phone"
- Beep sounds
- Automated greeting messages
- No human response after 3 seconds

IMMEDIATELY call the detected_answering_machine function.
After calling it, say the voicemail message EXACTLY as instructed, then end the call politely.
"""

        # Build the complete agent prompt by combining base prompt with specific instructions (language-specific)
        if language == "es":
            full_agent_prompt = f"""
Soy un agente. Sigue estas instrucciones cada vez que hables:

- El nombre del cliente es: {customer_first_name} (Usa el primer nombre ocasionalmente durante la conversaci├│n, NO en cada oraci├│n)
{context_info}
**INSTRUCCIONES BASE DEL AGENTE:**
{base_prompt}

**INSTRUCCIONES ESPEC├НFICAS DEL AGENTE PARA ESTA LLAMADA:**
{call_config.agent_prompt_preamble}
{previous_call_context}
{voicemail_instructions}
"""
        elif language == "fr":
            full_agent_prompt = f"""
Je suis un agent. Suivez ces instructions chaque fois que vous parlez:

- Le pr├йnom du client est: {customer_first_name} (Utilisez le pr├йnom occasionnellement pendant la conversation, PAS dans chaque phrase)
{context_info}
**INSTRUCTIONS DE BASE DE L'AGENT:**
{base_prompt}

**INSTRUCTIONS SP├ЙCIFIQUES DE L'AGENT POUR CET APPEL:**
{call_config.agent_prompt_preamble}
{previous_call_context}
{voicemail_instructions}
"""
        elif language == "hi":
            full_agent_prompt = f"""
рдореИрдВ рдПрдХ рдПрдЬреЗрдВрдЯ рд╣реВрдВред рд╣рд░ рдмрд╛рд░ рдмреЛрд▓рддреЗ рд╕рдордп рдЗрди рдирд┐рд░реНрджреЗрд╢реЛрдВ рдХрд╛ рдкрд╛рд▓рди рдХрд░реЗрдВ:

- рдЧреНрд░рд╛рд╣рдХ рдХрд╛ рдкрд╣рд▓рд╛ рдирд╛рдо рд╣реИ: {customer_first_name} (рдмрд╛рддрдЪреАрдд рдХреЗ рджреМрд░рд╛рди рдХрднреА-рдХрднрд╛рд░ рдкрд╣рд▓реЗ рдирд╛рдо рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ, рд╣рд░ рд╡рд╛рдХреНрдп рдореЗрдВ рдирд╣реАрдВ)
{context_info}
**рдПрдЬреЗрдВрдЯ рдХреЗ рдмреБрдирд┐рдпрд╛рджреА рдирд┐рд░реНрджреЗрд╢:**
{base_prompt}

**рдЗрд╕ рдХреЙрд▓ рдХреЗ рд▓рд┐рдП рд╡рд┐рд╢рд┐рд╖реНрдЯ рдПрдЬреЗрдВрдЯ рдирд┐рд░реНрджреЗрд╢:**
{call_config.agent_prompt_preamble}
{previous_call_context}
{voicemail_instructions}
"""
        elif language == "ar":
            full_agent_prompt = f"""
╪г┘Ж╪з ┘И┘Г┘К┘Д. ╪з╪к╪и╪╣ ┘З╪░┘З ╪з┘Д╪к╪╣┘Д┘К┘Е╪з╪к ┘Б┘К ┘Г┘Д ┘Е╪▒╪й ╪к╪к╪н╪п╪л ┘Б┘К┘З╪з:

- ╪з╪│┘Е ╪з┘Д╪╣┘Е┘К┘Д ╪з┘Д╪г┘И┘Д ┘З┘И: {customer_first_name} (╪з╪│╪к╪о╪п┘Е ╪з┘Д╪з╪│┘Е ╪з┘Д╪г┘И┘Д ╪г╪н┘К╪з┘Ж╪з┘Л ╪г╪л┘Ж╪з╪б ╪з┘Д┘Е╪н╪з╪п╪л╪й╪М ┘И┘Д┘К╪│ ┘Б┘К ┘Г┘Д ╪м┘Е┘Д╪й)
{context_info}
**╪к╪╣┘Д┘К┘Е╪з╪к ╪з┘Д┘И┘Г┘К┘Д ╪з┘Д╪г╪│╪з╪│┘К╪й:**
{base_prompt}

**╪к╪╣┘Д┘К┘Е╪з╪к ╪з┘Д┘И┘Г┘К┘Д ╪з┘Д┘Е╪н╪п╪п╪й ┘Д┘З╪░┘З ╪з┘Д┘Е┘Г╪з┘Д┘Е╪й:**
{call_config.agent_prompt_preamble}
{previous_call_context}
{voicemail_instructions}
"""
        else:
            full_agent_prompt = f"""
I am an agent. Follow these instructions every time you speak:

- Customer's first name is: {customer_first_name} (Use the first name occasionally during conversation, NOT in every sentence)
{context_info}
**BASE AGENT PROMPT INSTRUCTIONS:**
{base_prompt}

**SPECIFIC AGENT INSTRUCTIONS FOR THIS CALL:**
{call_config.agent_prompt_preamble}
{previous_call_context}
{voicemail_instructions}
"""

        logger.info(
            f"Agent initialized for customer: {customer_first_name}, language: {language}"
        )

        # Initialize knowledge base if enabled
        self.kb = None
        if call_config.use_knowledge_base:
            self.kb = KnowledgeBase(openai_api_key=call_config.model.api_key)
            if self.kb.enabled:
                logger.info("Knowledge base enabled for this call")

        # Choose TTS based on provider
        if call_config.tts.provider_name.lower() in ["openai", "open_ai"]:
            tts_instance = openai.TTS(voice="alloy")
            logger.info("Using OpenAI TTS")
        elif call_config.tts.provider_name.lower() in [
                "smallest", "smallest_ai", "smallestai"
        ]:
            os.environ["SMALLEST_API_KEY"] = call_config.tts.api_key

            tts_instance = smallestai.TTS(
                model=call_config.tts.model_id
                if call_config.tts.model_id else "lightning-large",
                voice_id=call_config.tts.voice_id
                if call_config.tts.voice_id else "irisha",
                api_key=call_config.tts.api_key,
                sample_rate=24000,
                speed=1.0,
                consistency=0.5,
                similarity=0.7,
                enhancement=0.0,
            )
            logger.info(
                f"Using Smallest.ai TTS with model: {call_config.tts.model_id or 'lightning-large'}, "
                f"voice_id: {call_config.tts.voice_id or 'irisha'}, sample_rate: 24000"
            )
        else:
            if language in ["es", "fr", "hi", "ar"]:
                elevenlabs_model = "eleven_multilingual_v2"
            else:
                elevenlabs_model = "eleven_turbo_v2_5"

            if call_config.tts.model_id and call_config.tts.model_id != elevenlabs_model:
                logger.info(
                    f"User provided model '{call_config.tts.model_id}' but using '{elevenlabs_model}' based on language '{language}'"
                )

            tts_instance = elevenlabs.TTS(
                voice_id=call_config.tts.voice_id,
                model=elevenlabs_model,
                api_key=call_config.tts.api_key,
                streaming_latency=2,
                chunk_length_schedule=[120, 160, 250, 290],
                enable_ssml_parsing=False,
            )
            logger.info(
                f"Using ElevenLabs TTS with model: {elevenlabs_model} (language: {language}), voice_id: {call_config.tts.voice_id}"
            )

        # Choose STT based on provider (use resolved language, not call_config.language)
        if call_config.stt.provider_name.lower() == "openai":
            # OpenAI STT - uses gpt-4o-transcribe model (recommended)
            # Uses OPENAI_API_KEY environment variable (already set above)
            openai_model = call_config.stt.model if call_config.stt.model not in [
                "nova-2", "nova", "deepgram"
            ] else "gpt-4o-transcribe"

            stt_instance = openai.STT(
                model=openai_model,
                language=language,  # Use resolved language (with fallback)
            )
            logger.info(
                f"Using OpenAI STT with model: {openai_model}, language: {language}"
            )
        elif call_config.stt.provider_name.lower() == "deepgram":
            # Deepgram STT (default)
            stt_instance = deepgram.STT(
                model=call_config.stt.model,
                api_key=call_config.stt.api_key,
                language=language,  # Use resolved language (with fallback)
            )
            logger.info(
                f"Using Deepgram STT with model: {call_config.stt.model}, language: {language}"
            )
        else:
            # Fallback to Deepgram if unknown provider
            logger.warning(
                f"Unknown STT provider '{call_config.stt.provider_name}', falling back to Deepgram"
            )

            # Use environment Deepgram key for fallback (don't reuse the invalid provider's key)
            deepgram_key = os.getenv("DEEPGRAM_API_KEY")
            if not deepgram_key:
                raise ValueError(
                    f"Unknown STT provider '{call_config.stt.provider_name}' and no DEEPGRAM_API_KEY found for fallback"
                )

            stt_instance = deepgram.STT(
                model="nova-2",
                api_key=deepgram_key,
                language=language,
            )
            logger.info(
                f"Using Deepgram STT (fallback) with model: nova-2, language: {language}"
            )

        # Create tools list (add RAG tool if knowledge base is enabled)
        agent_tools = []

        # Add voicemail detection tool if enabled
        if call_config.voicemail:
            # Language-specific default voicemail messages
            default_voicemail_messages = {
                "es":
                "Hola, devu├йlveme la llamada cuando recibas este mensaje.",
                "fr": "Bonjour, rappelez-moi quand vous recevrez ce message.",
                "hi": "рдирдорд╕реНрддреЗ, рдЬрдм рдЖрдкрдХреЛ рдпрд╣ рд╕рдВрджреЗрд╢ рдорд┐рд▓реЗ рддреЛ рдореБрдЭреЗ рд╡рд╛рдкрд╕ рдХреЙрд▓ рдХрд░реЗрдВред",
                "ar": "┘Е╪▒╪н╪и╪з┘Л╪М ╪з╪к╪╡┘Д ╪и┘К ╪╣┘Ж╪п┘Е╪з ╪к╪к┘Д┘В┘Й ┘З╪░┘З ╪з┘Д╪▒╪│╪з┘Д╪й.",
                "en": "Hi, call me back when you reach this message."
            }
            voicemail_msg = call_config.voicemail_message or default_voicemail_messages.get(
                language, default_voicemail_messages["en"])

            @function_tool()
            async def detected_answering_machine() -> str:
                """Call this function if you detect an answering machine or voicemail system.
                Listen for phrases like 'leave a message', 'at the tone', 'not available', beep sounds, or automated greetings."""
                return f"Voicemail detected. Say exactly: {voicemail_msg}"

            agent_tools.append(detected_answering_machine)
            logger.info(
                f"Voicemail detection enabled with message: {voicemail_msg[:50]}..."
            )

        # Capture knowledge base in local variable for type checker
        kb = self.kb
        if kb is not None and kb.enabled:
            # Create RAG function tool
            @function_tool()
            async def search_knowledge_base(query: str) -> str:
                """Search the company knowledge base for relevant information based on the user's question.
                Use this tool when the user asks questions about company information, products, services, or policies."""
                # Play typing sound while searching knowledge base (if enabled)
                if self._background_audio_started and self.background_audio:
                    try:
                        self.background_audio.play(
                            AudioConfig(BuiltinAudioClip.KEYBOARD_TYPING2,
                                        volume=0.5))
                        logger.info(
                            "тМия╕П Playing typing sound - searching knowledge base"
                        )
                    except Exception as e:
                        logger.debug(f"Failed to play typing sound: {e}")

                return await kb.search(query,
                                       top_k=call_config.knowledge_base_top_k)

            agent_tools.append(search_knowledge_base)
            logger.info("RAG function tool enabled for knowledge base queries")

        # Add call transfer tool if enabled
        if call_config.enable_call_transfer and call_config.transfer_phone_number:
            transfer_number = call_config.transfer_phone_number

            # Language-specific transfer confirmation messages
            transfer_confirmations = {
                "es": "Transfiriendo al agente humano...",
                "fr": "Transfert vers un agent humain...",
                "hi": "рдорд╛рдирд╡ рдПрдЬреЗрдВрдЯ рдХреЛ рд╕реНрдерд╛рдирд╛рдВрддрд░рд┐рдд рдХрд┐рдпрд╛ рдЬрд╛ рд░рд╣рд╛ рд╣реИ...",
                "ar": "╪м╪з╪▒┘Н ╪з┘Д╪к╪н┘И┘К┘Д ╪е┘Д┘Й ┘И┘Г┘К┘Д ╪и╪┤╪▒┘К...",
                "en": "Transferring to human agent..."
            }
            transfer_confirmation_msg = transfer_confirmations.get(
                language, transfer_confirmations["en"])

            @function_tool()
            async def transfer_to_human(
                reason: str = "User requested to speak with a human agent"
            ) -> str:
                """Transfer the call to a human agent when the user requests to speak with a real person or needs human assistance.
                Use this when the user explicitly asks to talk to someone, speak to a human, or get transferred to support."""
                if not self._agent_session:
                    # Language-specific unavailable messages
                    unavailable_messages = {
                        "es": "Transferencia no disponible en este momento",
                        "fr": "Transfert non disponible pour le moment",
                        "hi": "рд╕реНрдерд╛рдирд╛рдВрддрд░рдг рдЗрд╕ рд╕рдордп рдЙрдкрд▓рдмреНрдз рдирд╣реАрдВ рд╣реИ",
                        "ar": "╪з┘Д╪к╪н┘И┘К┘Д ╪║┘К╪▒ ┘Е╪к╪з╪н ┘Б┘К ╪з┘Д┘И┘В╪к ╪з┘Д╪н╪з┘Д┘К",
                        "en": "Transfer not available at this moment"
                    }
                    return unavailable_messages.get(language,
                                                    unavailable_messages["en"])

                await handle_transfer(self.room_name,
                                      self.participant_identity,
                                      transfer_number, self._agent_session,
                                      language)
                return transfer_confirmation_msg

            agent_tools.append(transfer_to_human)
            logger.info(f"Call transfer enabled to {transfer_number}")
        elif call_config.enable_call_transfer and not call_config.transfer_phone_number:
            logger.warning(
                "Call transfer enabled but no transfer_phone_number configured"
            )

        # Add end_call tool (always available for natural call endings)
        # Language-specific goodbye confirmation messages
        goodbye_messages = {
            "es": "Entendido. Gracias por tu tiempo. ┬бAdi├│s!",
            "fr": "Compris. Merci pour votre temps. Au revoir !",
            "hi": "рд╕рдордЭ рдЧрдпрд╛ред рдЖрдкрдХреЗ рд╕рдордп рдХреЗ рд▓рд┐рдП рдзрдиреНрдпрд╡рд╛рджред рдЕрд▓рд╡рд┐рджрд╛!",
            "ar": "┘Е┘Б┘З┘И┘Е. ╪┤┘Г╪▒╪з┘Л ┘Д┘И┘В╪к┘Г. ┘И╪п╪з╪╣╪з┘Л!",
            "en": "Understood. Thank you for your time. Goodbye!"
        }
        goodbye_msg = goodbye_messages.get(language, goodbye_messages["en"])

        @function_tool()
        async def end_call() -> str:
            """End the call gracefully when the conversation has naturally concluded.
            Use this when the user says goodbye, thanks, that's all, or when the objective is achieved and no further help is needed."""
            if not self._agent_session:
                logger.warning("end_call invoked but no active session")
                return "Call ending..."

            # Idempotent guard - only schedule deletion once
            if self._end_call_scheduled:
                logger.info(
                    "end_call already scheduled, skipping duplicate request")
                return goodbye_msg

            try:
                logger.info(
                    "ЁЯФЪ end_call invoked - scheduling room deletion after final message"
                )
                self._end_call_scheduled = True

                # Schedule room deletion after a brief delay to allow the goodbye message to be spoken
                async def delete_room_after_delay():
                    # Wait for the goodbye message to be spoken (2-3 seconds is typically enough)
                    await asyncio.sleep(3.0)

                    try:
                        # Delete the room to disconnect all participants
                        livekit_api_client = api.LiveKitAPI(
                            url=LIVEKIT_URL,
                            api_key=LIVEKIT_API_KEY,
                            api_secret=LIVEKIT_API_SECRET)

                        await livekit_api_client.room.delete_room(
                            api.DeleteRoomRequest(room=self.room_name))

                        await livekit_api_client.aclose()

                        logger.info(
                            f"тЬЕ Room {self.room_name} deleted successfully - all participants disconnected"
                        )
                    except Exception as e:
                        logger.error(f"тЭМ Error deleting room: {e}")
                        import traceback
                        logger.error(traceback.format_exc())

                # Schedule the deletion asynchronously so we can return immediately
                asyncio.create_task(delete_room_after_delay())

            except Exception as e:
                logger.error(f"тЭМ Error during end_call: {e}")
                import traceback
                logger.error(traceback.format_exc())

            return goodbye_msg

        agent_tools.append(end_call)
        logger.info("Natural call ending tool (end_call) enabled")

        # Add dynamic VAD tool for email input (always available)
        @function_tool()
        async def prepare_for_email_input() -> str:
            """MUST be called immediately BEFORE asking the user to spell their email address.
            This tool adjusts the audio sensors to allow for pauses between letters when spelling email.
            Call this BEFORE saying 'Can you spell your email' or 'Please provide your email address'."""
            if self._agent_session:
                # INCREASE delay to 3.0 seconds (allows long pauses between letters)
                self._agent_session.options.min_endpointing_delay = 3.0
                logger.info("ЁЯСВ VAD sensitivity lowered to 3.0s: Ready for email spelling with pauses")
                return "Audio sensors adjusted for email input. Now ask the user to spell their email letter by letter."
            return "Session not active."

        agent_tools.append(prepare_for_email_input)
        logger.info("Dynamic VAD tool (prepare_for_email_input) enabled")

        super().__init__(
            instructions=full_agent_prompt,
            stt=stt_instance,
            llm=openai.LLM(
                model=call_config.model.name,
                temperature=call_config.temperature,
            ),
            tts=tts_instance,
            tools=agent_tools,  # Add RAG tool
        )


def prewarm(proc: JobProcess):
    """Prewarm function to load VAD model"""
    proc.userdata["vad"] = silero.VAD.load()


async def analyze_tags_with_llm(
        full_transcript: str, user_tags: list[str], system_tags: list[str],
        call_duration_seconds: int, openai_api_key: str, current_utc_time: str
) -> tuple[list[str], list[str], bool, str | None]:
    """
    Use LLM to analyze which tags match the conversation content and detect callback requests
    
    Args:
        full_transcript: Full conversation transcript
        user_tags: List of user-defined tags to check
        system_tags: List of system-defined tags to check
        call_duration_seconds: Call duration in seconds
        openai_api_key: OpenAI API key
        current_utc_time: Current UTC time for callback time calculation
    
    Returns:
        Tuple of (user_tags_found, system_tags_found, callback_requested, callback_time)
    """
    # Skip if no tags to analyze
    if not user_tags and not system_tags:
        return [], [], False, None

    try:
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=openai_api_key)

        # Build analysis prompt
        prompt = f"""You are analyzing a phone conversation transcript to determine which tags are relevant and if a callback was requested.

Current UTC Time: {current_utc_time}
Call Duration: {call_duration_seconds} seconds ({call_duration_seconds // 60} minutes {call_duration_seconds % 60} seconds)

Transcript:
{full_transcript}

User Tags (check if the conversation topic/context matches):
{json.dumps(user_tags, indent=2)}

System Tags (check if the condition described in the tag is met):
{json.dumps(system_tags, indent=2)}

Analyze the transcript and determine:
1. Which tags apply (for user tags check conversation topics, for system tags check conditions)
2. Please make sure that user has asked for callback from agent that is user is busy or if he says to give a call afterwards only then callback_requested should be true not for the agents services.
3. If callback requested, extract the preferred time and round to nearest 15-minute interval (:00, :15, :30, or :45). Also make sure to ask which time zone is user talking about and then convert that time zone to UTC and then send UTC time zone in callback_time.

Respond with ONLY a JSON object in this exact format:
{{
  "user_tags_found": ["tag1", "tag2"],
  "system_tags_found": ["tag3"],
  "callback_requested": true,
  "callback_time": "2025-11-05T14:30:00Z"
}}

IMPORTANT:
- callback_time must be in UTC ISO format (YYYY-MM-DDTHH:MM:SSZ) with minutes at :00, :15, :30, or :45 only
- If no specific time mentioned, ask for timing or if no time he gives then suggest him reasonable time (e.g., next business day at 10:00 AM UTC)
- If no callback requested, set callback_requested to false and callback_time to null"""

        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role":
                "system",
                "content":
                "You are a precise conversation analyst. Respond only with valid JSON."
            }, {
                "role": "user",
                "content": prompt
            }],
            temperature=0.1,
            max_tokens=500)

        # Parse response with proper None checking
        message_content = response.choices[0].message.content
        if not message_content:
            logger.error("тЭМ OpenAI returned empty content for tag analysis")
            return [], [], False, None

        result_text = message_content.strip()

        # Extract JSON from response (handle markdown code blocks)
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split(
                "```")[0].strip()
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0].strip()

        result = json.loads(result_text)

        user_tags_found = result.get("user_tags_found", [])
        system_tags_found = result.get("system_tags_found", [])
        callback_requested = result.get("callback_requested", False)
        callback_time = result.get("callback_time", None)

        logger.info(
            f"ЁЯП╖я╕П  Tag analysis: user={user_tags_found}, system={system_tags_found}, callback={callback_requested}, time={callback_time}"
        )

        return user_tags_found, system_tags_found, callback_requested, callback_time

    except Exception as e:
        logger.error(f"тЭМ Failed to analyze tags with LLM: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return [], [], False, None


async def entrypoint(ctx: JobContext):
    """Agent entrypoint for handling voice calls"""
    logger.info(f"Agent starting for room: {ctx.room.name}")

    call_start_time = datetime.utcnow()
    call_id = ctx.room.name

    call_config = load_call_config(ctx.room.name)
    if not call_config:
        logger.warning(f"No config found for room {ctx.room.name}")

        try:
            metadata = json.loads(ctx.job.metadata) if ctx.job.metadata else {}
            phone_number = metadata.get("phone_number")
            contact_name = metadata.get("contact_name", "User")
            user_speak_first = metadata.get("user_speak_first", True)

            logger.info(
                f"Using metadata: phone={phone_number}, name={contact_name}")

            from src.models import TTSConfig, STTConfig, ModelConfig
            call_config = CallConfig(
                to_phone=phone_number or "+1234567890",
                from_phone="+0987654321",
                twilio_account_sid="default",
                twilio_auth_token="default",
                contact_name=contact_name,
                agent_initial_message=
                f"Hello {contact_name}! How can I help you today?",
                agent_prompt_preamble="You are a helpful assistant.",
                voicemail_message="Please call back",
                user_speak_first=user_speak_first,
                webhook_url=None,
                previous_call_summary=None,
                current_date=None,
                current_time=None,
                enable_call_transfer=False,
                transfer_phone_number=None,
                livekit_sip_trunk_id=os.getenv("LIVEKIT_SIP_TRUNK_ID", ""),
                tts=TTSConfig(provider_name="eleven_labs",
                              voice_id="21m00Tcm4TlvDq8ikWAM",
                              model_id="eleven_turbo_v2_5",
                              api_key=os.getenv("ELEVEN_API_KEY", "")),
                stt=STTConfig(provider_name="deepgram",
                              model="nova-2",
                              api_key=os.getenv("DEEPGRAM_API_KEY", "")),
                model=ModelConfig(name="gpt-4o-mini",
                                  api_key=os.getenv("OPENAI_API_KEY", "")),
            )
        except Exception as e:
            logger.error(f"Error parsing metadata: {e}")
            raise

    # Start call recording BEFORE connecting (LiveKit requirement)
    recording_info = None
    if call_config.recording:
        gcs_bucket = os.getenv("GCS_BUCKET_NAME")
        gcs_credentials = os.getenv("GCS_SERVICE_ACCOUNT_JSON")

        if gcs_bucket and gcs_credentials:
            logger.info("ЁЯУ╣ Recording enabled - starting room recording")
            try:
                # Generate GCS file path
                date_prefix = datetime.now().strftime("%Y/%m/%d")
                timestamp = datetime.now().strftime("%H%M%S")
                gcs_filename = f"recordings/{date_prefix}/{call_id}_{timestamp}.mp4"

                # Set up recording to Google Cloud Storage
                req = api.RoomCompositeEgressRequest(
                    room_name=ctx.room.name,
                    audio_only=True,
                    file_outputs=[
                        api.EncodedFileOutput(
                            file_type=api.EncodedFileType.MP4,
                            filepath=gcs_filename,
                            gcp=api.GCPUpload(
                                credentials=gcs_credentials,
                                bucket=gcs_bucket,
                            ),
                        )
                    ],
                )

                lkapi = api.LiveKitAPI()
                egress_info = await lkapi.egress.start_room_composite_egress(
                    req)
                await lkapi.aclose()

                recording_info = {
                    "egress_id": egress_info.egress_id,
                    "gcs_filename": gcs_filename,
                    "gcs_bucket": gcs_bucket
                }

                logger.info(
                    f"тЬЕ Recording started: egress_id={egress_info.egress_id}, path={gcs_filename}"
                )

            except Exception as e:
                logger.error(f"тЭМ Failed to start recording: {e}")
                import traceback
                logger.error(traceback.format_exc())
        else:
            logger.warning(
                "тЪая╕П Recording enabled but GCS credentials not configured")

    # NOW connect to the room
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

    # Register room event listeners for SIP diagnostics
    @ctx.room.on("participant_attributes_changed")
    def on_attributes_changed(changed_attributes, participant_obj):
        """Log all SIP attribute changes in real-time for debugging"""
        sip_attrs = {k: v for k, v in changed_attributes.items() if k.startswith("sip.")}
        if sip_attrs:
            logger.info(f"ЁЯФД SIP attribute change for {participant_obj.identity}: {sip_attrs}")
            logger.info(f"   All current attributes: {dict(participant_obj.attributes)}")

    @ctx.room.on("participant_disconnected")
    def on_participant_disconnected(participant_obj):
        """Log when a participant disconnects with all available SIP info"""
        logger.warning(f"ЁЯСЛ Participant disconnected: {participant_obj.identity}")
        logger.warning(f"   Kind: {participant_obj.kind}")
        logger.warning(f"   Final attributes: {dict(participant_obj.attributes)}")
        sip_status = participant_obj.attributes.get("sip.callStatus", "unknown")
        sip_code = participant_obj.attributes.get("sip.statusCode", "unknown")
        sip_reason = participant_obj.attributes.get("sip.disconnectReason", "unknown")
        logger.warning(f"   SIP status={sip_status}, code={sip_code}, reason={sip_reason}")

    # Wait for SIP participant to join
    participant = await ctx.wait_for_participant()
    logger.info(
        f"ЁЯУЮ Participant {participant.identity} joined - waiting for pickup...")
    logger.info(f"   Initial attributes: {dict(participant.attributes)}")

    # Monitor sip.callStatus to detect when user actually picks up
    # This prevents the agent from speaking before the user answers
    if participant.kind == rtc.ParticipantKind.PARTICIPANT_KIND_SIP:
        logger.info(
            "тП│ Monitoring sip.callStatus - waiting for 'active' status...")

        # Wait for call status to become "active" (user picked up)
        # Also detect failures: if status changes to a terminal state or
        # the participant disconnects, stop waiting and handle gracefully.
        max_wait_seconds = 120  # Max 2 minutes for ringing/dialing before giving up
        elapsed = 0.0
        call_connected = False

        while elapsed < max_wait_seconds:
            current_status = participant.attributes.get("sip.callStatus", "unknown")

            if current_status == "active":
                call_connected = True
                break

            # Check for terminal SIP states that indicate call failure
            if current_status in ("disconnected", "hangup", "failed", "error", "automation"):
                logger.warning(
                    f"тЭМ SIP call ended during dialing with status: {current_status}")
                # Log any SIP disconnect reason if available
                disconnect_reason = participant.attributes.get("sip.disconnectReason", "unknown")
                sip_code = participant.attributes.get("sip.statusCode", "unknown")
                logger.warning(
                    f"   SIP disconnect reason: {disconnect_reason}, SIP code: {sip_code}")
                logger.warning(f"   All SIP attributes: {dict(participant.attributes)}")
                break

            # Check if participant is still in the room
            if participant.identity not in [
                p.identity for p in ctx.room.remote_participants.values()
            ]:
                logger.warning(
                    f"тЭМ SIP participant {participant.identity} left the room while still '{current_status}'")
                logger.warning(f"   Last known SIP attributes: {dict(participant.attributes)}")
                break

            await asyncio.sleep(0.5)
            elapsed += 0.5

            # Log status periodically (every 5 seconds instead of every 0.5s)
            if int(elapsed * 2) % 10 == 0:
                logger.info(
                    f"тП│ SIP call status: {current_status} (waiting {elapsed:.0f}s)")

        if not call_connected:
            logger.error(f"тЭМ SIP call was NOT answered after {elapsed:.0f}s")
            logger.error(f"   Last sip.callStatus: {participant.attributes.get('sip.callStatus', 'unknown')}")
            logger.error(f"   All participant attributes: {dict(participant.attributes)}")

            # Send webhook notification about failed call
            if call_config.webhook_url:
                try:
                    call_end_time = datetime.utcnow()
                    duration_seconds = int(
                        (call_end_time - call_start_time).total_seconds())
                    await webhook_sender.send_call_ended(
                        call_config.webhook_url,
                        call_id,
                        duration_seconds,
                        call_start_time,
                        call_end_time,
                        is_voicemail=False,
                        is_rejected=True,
                        call_outcome="not_answered",
                        end_reason=f"SIP call failed: {participant.attributes.get('sip.callStatus', 'unknown')}"
                    )
                except Exception as e:
                    logger.error(f"Failed to send call-failed webhook: {e}")

            # Clean up - shut down the agent since the call never connected
            logger.info("ЁЯФЪ Shutting down agent - call was not answered")
            ctx.shutdown()
            return

        logger.info("тЬЕ Call is now active - user picked up!")

        # Additional buffer for audio stability and initial message delay
        await asyncio.sleep(3.0)
    else:
        # Non-SIP participant, use old behavior
        logger.info("тП│ Waiting for audio track to stabilize...")
        await asyncio.sleep(2.0)

    logger.info(f"тЬЕ Call fully connected, starting voice assistant")

    # Send PHONE_CALL_CONNECTED webhook
    if call_config.webhook_url:
        await webhook_sender.send_call_connected(call_config.webhook_url,
                                                 call_id, call_start_time)
    else:
        logger.info("No webhook_url configured - webhooks disabled")

    session = AgentSession(
        vad=ctx.proc.userdata["vad"],
        min_endpointing_delay=0.5,
        max_endpointing_delay=6.0,
    )

    # Track full transcript for TRANSCRIPT_COMPLETE webhook
    call_transcript = []

    # Setup transcript handler
    @session.on("conversation_item_added")
    def on_conversation_item(event):
        """Send webhook when conversation item is added (both user and agent)"""
        item = event.item
        logger.info(
            f"Conversation item: role={item.role}, content={item.text_content[:50] if item.text_content else 'None'}..."
        )

        if item.text_content:
            # Map all possible agent roles to "bot"
            if item.role in ["assistant", "agent", "system"]:
                sender = "bot"
            else:
                sender = "user"
                
                # RESET VAD SENSITIVITY only if we're NOT in email collection mode
                # Check if the last bot message was about email spelling
                if session.options.min_endpointing_delay > 1.0:
                    # Look at recent transcript to see if we're still collecting email
                    still_collecting_email = False
                    if len(call_transcript) > 0:
                        # Check last 2 bot messages for email-related keywords
                        recent_bot_messages = [
                            msg['text'].lower() for msg in call_transcript[-3:]
                            if msg['sender'] == 'bot'
                        ]
                        email_keywords = ['spell', 'email', 'letter by letter', 'at the rate', 'dot com', 'dot', '@']
                        still_collecting_email = any(
                            any(keyword in msg for keyword in email_keywords)
                            for msg in recent_bot_messages
                        )
                    
                    # Only reset if we're clearly done with email collection
                    if not still_collecting_email:
                        session.options.min_endpointing_delay = 0.5
                        logger.info("ЁЯСВ VAD sensitivity restored to normal (0.5s) - email collection complete")

                # Detect if user is providing structured data (email, phone, appointment details)
                # Play typing sound to indicate agent is noting it down
                text_lower = item.text_content.lower()
                data_keywords = [
                    '@',
                    'dot com',
                    'dot org',
                    'gmail',
                    'yahoo',
                    'hotmail',  # Email indicators
                    'appointment',
                    'schedule',
                    'meeting',
                    'calendar',  # Appointment indicators
                    'phone',
                    'number',
                    'call me',
                    'contact',  # Phone indicators
                    'name is',
                    'my name',
                    "i'm",
                    'called',  # Name indicators
                    'address',
                    'street',
                    'city',
                    'zip',  # Address indicators
                ]

                if any(keyword in text_lower for keyword in data_keywords):
                    # User is providing important data - play typing sound (if enabled)
                    if assistant._background_audio_started and assistant.background_audio:
                        try:
                            assistant.background_audio.play(
                                AudioConfig(BuiltinAudioClip.KEYBOARD_TYPING,
                                            volume=0.6))
                            logger.info(
                                "тМия╕П Playing typing sound - user providing data"
                            )
                        except Exception as e:
                            logger.debug(f"Failed to play typing sound: {e}")

            # Add to transcript history
            call_transcript.append({
                "sender": sender,
                "text": item.text_content
            })

            # Send live transcript webhook
            if call_config.webhook_url:
                logger.info(f"Sending transcript webhook [{sender}]")
                asyncio.create_task(
                    webhook_sender.send_live_transcript(
                        call_config.webhook_url,
                        call_id,
                        item.text_content,
                        sender,
                        is_partial=
                        True  # True because these are live/partial transcripts during ongoing call
                    ))

    assistant = VoiceAssistant(call_config=call_config,
                               room_name=ctx.room.name,
                               participant_identity=participant.identity)
    assistant._agent_session = session

    await session.start(
        room=ctx.room,
        agent=assistant,
        participant=participant,  # Critical: tells the session which participant's audio to listen to
    )

    logger.info("Voice agent started successfully")

    # Start background audio player for typing sounds (only if enabled)
    if assistant.background_audio:
        try:
            await assistant.background_audio.start(room=ctx.room,
                                                   agent_session=session)
            assistant._background_audio_started = True
            logger.info(
                "ЁЯО╡ Background audio player started (typing sounds enabled)")
        except Exception as e:
            logger.warning(f"тЪая╕П Failed to start background audio player: {e}")

    # Send initial greeting if agent should speak first
    if not call_config.user_speak_first:
        # No additional delay needed - we already waited for sip.callStatus="active" + 1.5s buffer
        logger.info("ЁЯдЦ Agent will speak first - delivering initial message...")

        # Replace placeholders in initial message (support various case formats)
        initial_message = call_config.agent_initial_message
        placeholders = [
            "{customer name}", "{Customer Name}", "{CUSTOMER NAME}",
            "{contact_name}", "{Contact_Name}", "{CONTACT_NAME}",
            "{customer_name}", "{Customer_name}"
        ]
        for placeholder in placeholders:
            initial_message = initial_message.replace(placeholder,
                                                      call_config.contact_name)

        logger.info(f"ЁЯдЦ Agent speaking: {initial_message[:50]}...")

        # Use session.say() with allow_interruptions=False for outbound calls
        # This ensures the agent completes the entire initial message without being cut off
        await session.say(initial_message, allow_interruptions=False)

    # Keep running and handle disconnect
    async def handle_disconnect():
        """Handle call end webhooks"""
        call_end_time = datetime.utcnow()
        duration_seconds = int(
            (call_end_time - call_start_time).total_seconds())
        logger.info(f"Call ended. Duration: {duration_seconds}s")

        # Wait for recording to complete and get URL if recording was enabled
        recording_url = None
        if recording_info:
            logger.info(
                "ЁЯУ╣ Waiting for recording to complete and upload to GCS...")
            try:
                # Wait for recording to finish uploading and generate signed URL
                recording_url = await recording_manager.wait_for_recording_completion(
                    egress_id=recording_info['egress_id'],
                    gcs_filename=recording_info['gcs_filename'],
                    max_wait_seconds=60,
                    poll_interval=2.0)
                if recording_url:
                    logger.info(
                        f"тЬЕ Recording URL ready: {recording_url[:100]}...")
                else:
                    logger.warning(
                        "тЪая╕П Recording URL not available (timeout or failed)")
            except Exception as e:
                logger.error(f"тЭМ Error waiting for recording: {e}")

        # Send PHONE_CALL_ENDED webhook with recording URL
        if call_config.webhook_url:
            try:
                await webhook_sender.send_call_ended(
                    call_config.webhook_url,
                    call_id,
                    duration_seconds,
                    call_start_time,
                    call_end_time,
                    is_voicemail=False,
                    is_rejected=False,
                    call_outcome="completed",
                    end_reason="unknown",
                    recording_url=recording_url)
            except Exception as e:
                logger.error(f"Error sending end webhook: {e}")

        # Build full transcript and send TRANSCRIPT_COMPLETE webhook
        if call_config.webhook_url:
            try:
                # Format full transcript as "BOT: text\nUSER: text\n..."
                transcript_lines = []
                for item in call_transcript:
                    sender_label = item['sender'].upper()
                    transcript_lines.append(f"{sender_label}: {item['text']}")

                full_transcript = "\n".join(transcript_lines)

                # Prepare recording URLs as a list
                recording_urls = [recording_url] if recording_url else []

                # Analyze tags and callback information with LLM if tags are provided
                user_tags_found = []
                system_tags_found = []
                callback_requested = False
                callback_time = None

                if call_config.user_tags or call_config.system_tags:
                    logger.info(
                        f"ЁЯП╖я╕П  Analyzing tags: user={call_config.user_tags}, system={call_config.system_tags}"
                    )
                    current_utc = datetime.utcnow().strftime(
                        "%Y-%m-%dT%H:%M:%SZ")
                    user_tags_found, system_tags_found, callback_requested, callback_time_str = await analyze_tags_with_llm(
                        full_transcript=full_transcript,
                        user_tags=call_config.user_tags,
                        system_tags=call_config.system_tags,
                        call_duration_seconds=duration_seconds,
                        openai_api_key=call_config.model.api_key,
                        current_utc_time=current_utc)

                    # Convert callback_time string to datetime object
                    if callback_time_str:
                        try:
                            callback_time = datetime.fromisoformat(
                                callback_time_str.replace('Z', '+00:00'))
                        except Exception as e:
                            logger.error(f"тЭМ Error parsing callback_time: {e}")
                            callback_time = None

                logger.info(
                    f"ЁЯУЭ Sending transcript complete webhook with {len(call_transcript)} items"
                )

                # Calculate call costs
                cost_breakdown_dict = None
                try:
                    calculator = CostCalculator()

                    # Count TTS characters (all bot messages)
                    tts_chars = sum(
                        len(item['text']) for item in call_transcript
                        if item['sender'] == 'bot')

                    # Determine calling provider (voxsun or twilio)
                    calling_provider = "voxsun"  # Default to voxsun
                    trunk_id = call_config.livekit_sip_trunk_id if hasattr(
                        call_config, 'livekit_sip_trunk_id') else ""
                    if "twilio" in trunk_id.lower():
                        calling_provider = "twilio"

                    # Calculate total cost
                    cost_breakdown = calculator.calculate_total_cost(
                        call_duration_seconds=duration_seconds,
                        tts_provider=call_config.tts.provider_name,
                        tts_model_id=call_config.tts.model_id,
                        stt_provider=call_config.stt.provider_name,
                        stt_model=call_config.stt.model,
                        llm_model=call_config.model.name,
                        calling_provider=calling_provider)

                    # Override TTS chars with actual count from transcript
                    calculator.tts_chars_sent = tts_chars
                    cost_breakdown.tts_cost = calculator.calculate_tts_cost(
                        tts_chars, call_config.tts.provider_name,
                        call_config.tts.model_id)
                    cost_breakdown.total_cost = (
                        cost_breakdown.calling_provider_cost +
                        cost_breakdown.tts_cost + cost_breakdown.stt_cost +
                        cost_breakdown.llm_cost)

                    cost_breakdown_dict = cost_breakdown.to_dict()
                    logger.info(
                        f"ЁЯТ░ Call cost calculated: ${cost_breakdown.total_cost:.4f}"
                    )

                except Exception as e:
                    logger.error(f"тЭМ Error calculating costs: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

                await webhook_sender.send_transcript_complete(
                    call_config.webhook_url,
                    call_id,
                    full_transcript,
                    recording_urls,
                    user_tags_found=user_tags_found,
                    system_tags_found=system_tags_found,
                    callback_requested=callback_requested,
                    callback_time=callback_time,
                    cost_breakdown=cost_breakdown_dict)
            except Exception as e:
                logger.error(f"Error sending transcript complete webhook: {e}")

    # Register shutdown callback to send webhooks before process exits
    ctx.add_shutdown_callback(handle_disconnect)


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
        ), )
